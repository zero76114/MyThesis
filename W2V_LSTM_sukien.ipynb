{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM IMDB Movie Review Tutorial\n",
    "Josiah Olson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import time\n",
    "import numpy as np\n",
    "np.random.seed(1337)  # for reproducibility\n",
    "\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers import Dense, Dropout, Embedding, LSTM, Input, merge, BatchNormalization, GRU\n",
    "from keras.datasets import imdb\n",
    "\n",
    "import os\n",
    "from keras.preprocessing.text import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "max_features = 10000\n",
    "max_len = 30  # cut texts after this number of words (among top max_features most common words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x:\n",
      "['\\xef\\xbb\\xbfABI: Ng\\xc3\\xa0y GDKHQ T\\xe1\\xbb\\x95 ch\\xe1\\xbb\\xa9c \\xc4\\x90\\xe1\\xba\\xa1i h\\xe1\\xbb\\x99i \\xc4\\x91\\xe1\\xbb\\x93ng c\\xe1\\xbb\\x95 \\xc4\\x91\\xc3\\xb4ng th\\xc6\\xb0\\xe1\\xbb\\x9dng ni\\xc3\\xaan n\\xc4\\x83m 2014 v\\xc3\\xa0 T\\xe1\\xba\\xa1m \\xe1\\xbb\\xa9ng c\\xe1\\xbb\\x95 t\\xe1\\xbb\\xa9c n\\xc4\\x83m 2013 b\\xe1\\xba\\xb1ng ti\\xe1\\xbb\\x81n (10%)\\n']\n",
      "['\\xef\\xbb\\xbfPGT: Ng\\xc3\\xa0y GDKHQ tham d\\xe1\\xbb\\xb1 \\xc4\\x90\\xe1\\xba\\xa1i h\\xe1\\xbb\\x99i \\xc4\\x91\\xe1\\xbb\\x93ng c\\xe1\\xbb\\x95 \\xc4\\x91\\xc3\\xb4ng th\\xc6\\xb0\\xe1\\xbb\\x9dng ni\\xc3\\xaan 2016\\n']\n",
      "2780\n",
      "y:\n",
      "[1]\n",
      "[0]\n",
      "2780\n"
     ]
    }
   ],
   "source": [
    "# get dataset and unzip: http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
    "\n",
    "X_train = []\n",
    "y_train = []\n",
    "\n",
    "path = 'C:/1_Research/Create_data/aclImdb/train_sukien/pos/'\n",
    "X_train.extend([open(path + f).read() for f in os.listdir(path) if f.endswith('.txt')])\n",
    "y_train.extend([1 for _ in range(1998)])\n",
    "\n",
    "path = 'C:/1_Research/Create_data/aclImdb/train_sukien/neg/'\n",
    "X_train.extend([open(path + f).read() for f in os.listdir(path) if f.endswith('.txt')])\n",
    "y_train.extend([0 for _ in range(782)])\n",
    "\n",
    "print('x:')\n",
    "print(X_train[:1])\n",
    "print(X_train[-1:])\n",
    "print(len(X_train))\n",
    "print('y:')\n",
    "print(y_train[:1])\n",
    "print(y_train[-1:])\n",
    "print(len(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x:\n",
      "['\\xef\\xbb\\xbfSPD: Ng\\xc3\\xa0y GDKHQ Chi tr\\xe1\\xba\\xa3 c\\xe1\\xbb\\x95 t\\xe1\\xbb\\xa9c b\\xe1\\xba\\xb1ng ti\\xe1\\xbb\\x81n (8%)\\n']\n",
      "['\\xef\\xbb\\xbfYBC: Ng\\xc3\\xa0y GDKHQ tham d\\xe1\\xbb\\xb1 \\xc4\\x90\\xe1\\xba\\xa1i h\\xe1\\xbb\\x99i \\xc4\\x91\\xe1\\xbb\\x93ng c\\xe1\\xbb\\x95 \\xc4\\x91\\xc3\\xb4ng th\\xc6\\xb0\\xe1\\xbb\\x9dng ni\\xc3\\xaan 2016\\n']\n",
      "1390\n",
      "y:\n",
      "[1]\n",
      "[0]\n",
      "1390\n"
     ]
    }
   ],
   "source": [
    "# read in the test data\n",
    "\n",
    "X_test = []\n",
    "y_test = []\n",
    "\n",
    "path = 'C:/1_Research/Create_data/aclImdb/test_sukien/pos/'\n",
    "X_test.extend([open(path + f).read() for f in os.listdir(path) if f.endswith('.txt')])\n",
    "y_test.extend([1 for _ in range(999)])\n",
    "\n",
    "path = 'C:/1_Research/Create_data/aclImdb/test_sukien/neg/'\n",
    "X_test.extend([open(path + f).read() for f in os.listdir(path) if f.endswith('.txt')])\n",
    "y_test.extend([0 for _ in range(391)])\n",
    "\n",
    "print('x:')\n",
    "print(X_test[:1])\n",
    "print(X_test[-1:])\n",
    "print(len(X_test))\n",
    "print('y:')\n",
    "print(y_test[:1])\n",
    "print(y_test[-1:])\n",
    "print(len(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#tokenize works to list of integers where each integer is a key to a word\n",
    "imdbTokenizer = Tokenizer(nb_words=max_features)\n",
    "\n",
    "imdbTokenizer.fit_on_texts(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 đông\n",
      "3 gdkhq\n",
      "8 Đại\n",
      "4 năm\n",
      "19 dự\n",
      "12 tức\n",
      "15 2015\n",
      "13 2014\n",
      "1 cổ\n",
      "6 thường\n",
      "7 hội\n",
      "2 ngày\n",
      "10 đồng\n",
      "16 chức\n",
      "18 trả\n",
      "17 tổ\n",
      "14 tiền\n",
      "11 bằng\n",
      "9 niên\n"
     ]
    }
   ],
   "source": [
    "#print top 20 words \n",
    "#note zero is reserved for non frequent words\n",
    "for word, value in imdbTokenizer.word_index.items():\n",
    "    if value < 20:\n",
    "        print(value, word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cổ\n",
      "ngày\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "#create int to word dictionary\n",
    "intToWord = {}\n",
    "for word, value in imdbTokenizer.word_index.items():\n",
    "    intToWord[value] = word\n",
    "\n",
    "#add a symbol for null placeholder\n",
    "intToWord[0] = \"!!!NA!!!\"\n",
    "    \n",
    "print(intToWord[1])\n",
    "print(intToWord[2])\n",
    "print(intToWord[32])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "﻿ABI: Ngày GDKHQ Tổ chức Đại hội đồng cổ đông thường niên năm 2014 và Tạm ứng cổ tức năm 2013 bằng tiền (10%)\n",
      "\n",
      "[[596, 2, 3, 17, 16, 8, 7, 10, 1, 5, 6, 9, 4, 13, 27, 24, 23, 1, 12, 4, 26, 11, 14, 32]]\n",
      "﻿abi\n",
      "ngày\n",
      "gdkhq\n",
      "tổ\n",
      "chức\n",
      "Đại\n",
      "hội\n",
      "đồng\n",
      "cổ\n",
      "đông\n",
      "thường\n",
      "niên\n",
      "năm\n",
      "2014\n",
      "và\n",
      "tạm\n",
      "ứng\n",
      "cổ\n",
      "tức\n",
      "năm\n",
      "2013\n",
      "bằng\n",
      "tiền\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "#convert word strings to integer sequence lists\n",
    "print(X_train[0])\n",
    "print(imdbTokenizer.texts_to_sequences(X_train[:1]))\n",
    "for value in imdbTokenizer.texts_to_sequences(X_train[:1])[0]:\n",
    "    print(intToWord[value])\n",
    "    \n",
    "X_train = imdbTokenizer.texts_to_sequences(X_train)\n",
    "X_test = imdbTokenizer.texts_to_sequences(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2780 train sequences\n",
      "1390 test sequences\n",
      "Pad sequences (samples x time)\n",
      "X_train shape: (2780L, 30L)\n",
      "X_test shape: (1390L, 30L)\n"
     ]
    }
   ],
   "source": [
    "# Censor the data by having a max review length (in number of words)\n",
    "\n",
    "#use this function to load data from keras pickle instead of munging as shown above\n",
    "#(X_train, y_train), (X_test, y_test) = imdb.load_data(nb_words=max_features,\n",
    "#                                                      test_split=0.2)\n",
    "\n",
    "print(len(X_train), 'train sequences')\n",
    "print(len(X_test), 'test sequences')\n",
    "\n",
    "print(\"Pad sequences (samples x time)\")\n",
    "X_train = sequence.pad_sequences(X_train, maxlen=max_len)\n",
    "X_test = sequence.pad_sequences(X_test, maxlen=max_len)\n",
    "print('X_train shape:', X_train.shape)\n",
    "print('X_test shape:', X_test.shape)\n",
    "y_train = np.array(y_train)\n",
    "y_test = np.array(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: [  0   0   0   0   0   0 596   2   3  17  16   8   7  10   1   5   6   9\n",
      "   4  13  27  24  23   1  12   4  26  11  14  32]\n",
      "y: 1\n"
     ]
    }
   ],
   "source": [
    "#example of a sentence sequence, note that lower integers are words that occur more commonly\n",
    "print(\"x:\", X_train[0]) #per observation vector of 20000 words\n",
    "print(\"y:\", y_train[0]) #positive or negative review encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y distribution: (array([0, 1]), array([ 782, 1998], dtype=int64))\n",
      "max x word: 1101 ; min x word 0\n",
      "y distribution test: (array([0, 1]), array([391, 999], dtype=int64))\n",
      "max x word test: 1070 ; min x word 0\n"
     ]
    }
   ],
   "source": [
    "# double check that word sequences behave/final dimensions are as expected\n",
    "print(\"y distribution:\", np.unique(y_train, return_counts=True))\n",
    "print(\"max x word:\", np.max(X_train), \"; min x word\", np.min(X_train))\n",
    "print(\"y distribution test:\", np.unique(y_test, return_counts=True))\n",
    "print(\"max x word test:\", np.max(X_test), \"; min x word\", np.min(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "most and least popular words: \n",
      "(array([   0,    1,    2, ..., 1099, 1100, 1101]), array([43926,  3138,  2732, ...,     1,     1,     1], dtype=int64))\n"
     ]
    }
   ],
   "source": [
    "print(\"most and least popular words: \")\n",
    "print(np.unique(X_train, return_counts=True))\n",
    "# as expected zero is the highly used word for words not in index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#set model hyper parameters\n",
    "epochs = 16\n",
    "embedding_neurons = 128\n",
    "lstm_neurons = 64\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next let's import pre-trained word vectors from google and use them to initialize our embedding to see if this improves the neural net's performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda2\\lib\\site-packages\\gensim\\utils.py:840: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n",
      "C:\\Anaconda2\\lib\\site-packages\\gensim\\utils.py:1015: UserWarning: Pattern library is not installed, lemmatization won't be available.\n",
      "  warnings.warn(\"Pattern library is not installed, lemmatization won't be available.\")\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "#get pre trained word2vec from google:\n",
    "#https://doc-0k-4g-docs.googleusercontent.com/docs/securesc/gnqvgap6hjncpd3b10i2tv865io48jas/hmjtdgee48c14e1parufukrpkb8urra5/1463018400000/06848720943842814915/09676831593570546402/0B7XkCwpI5KDYNlNUTTlSS21pQmM?e=download&nonce=4l49745nmtine&user=09676831593570546402&hash=i2qe9mshan4mesl112ct9bu1tj9kr1hq\n",
    "\n",
    "googlew2v = Word2Vec.load_word2vec_format('C:/1_Research/Create_data/aclImdb/word2vector/GoogleNews-vectors-negative300.bin', binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.26202468  0.15868397  0.27812652 ...,  0.10937801  0.75767728\n",
      "   0.4428527 ]\n",
      " [ 0.96970407  0.12253514  0.35782362 ...,  0.97491444  0.6667405\n",
      "   0.40817497]\n",
      " [ 0.34841225  0.36222138  0.90096367 ...,  0.47904751  0.85001801\n",
      "   0.84470638]\n",
      " ..., \n",
      " [ 0.37284314  0.53258785  0.26119324 ...,  0.88342727  0.14764413\n",
      "   0.41580886]\n",
      " [ 0.83421612  0.39597201  0.98788126 ...,  0.01348185  0.10617085\n",
      "   0.5794498 ]\n",
      " [ 0.18089266  0.15663464  0.0585371  ...,  0.95018796  0.15530301\n",
      "   0.5283831 ]]\n",
      "(10000L, 300L)\n"
     ]
    }
   ],
   "source": [
    "# get word vectors for words in my index\n",
    "googleVecs = []\n",
    "for value in range(max_features):\n",
    "    try:\n",
    "        googleVecs.append(googlew2v[intToWord[value]])\n",
    "    except:\n",
    "        googleVecs.append(np.random.uniform(size=300))\n",
    "\n",
    "googleVecs = np.array(googleVecs)\n",
    "\n",
    "print(googleVecs)\n",
    "print(googleVecs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "input_1 (InputLayer)             (None, 30)            0                                            \n",
      "____________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)          (None, 30, 300)       3000000     input_1[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "batchnormalization_1 (BatchNormal(None, 30, 300)       600         embedding_1[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "gru_1 (GRU)                      (None, 64)            70080       batchnormalization_1[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "gru_2 (GRU)                      (None, 64)            70080       batchnormalization_1[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "merge_1 (Merge)                  (None, 128)           0           gru_1[0][0]                      \n",
      "                                                                   gru_2[0][0]                      \n",
      "____________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)              (None, 128)           0           merge_1[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "dense_1 (Dense)                  (None, 1)             129         dropout_1[0][0]                  \n",
      "====================================================================================================\n",
      "Total params: 3140889\n",
      "____________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Bi-directional google\n",
    "\n",
    "# this example tests if using pretrained embeddings will improve performance \n",
    "# relative to starting with random embeddings\n",
    "\n",
    "# this is the placeholder tensor for the input sequences\n",
    "sequence = Input(shape=(max_len,), dtype='int32')\n",
    "# this embedding layer will transform the sequences of integers\n",
    "# into vectors of size embedding\n",
    "# embedding layer converts dense int input to one-hot in real time to save memory\n",
    "embedded = Embedding(max_features, 300, input_length=max_len, weights=[googleVecs])(sequence)\n",
    "# normalize embeddings by input/word in sentence\n",
    "bnorm = BatchNormalization()(embedded)\n",
    "\n",
    "# apply forwards LSTM layer size lstm_neurons\n",
    "forwards = GRU(lstm_neurons, dropout_W=0.8, dropout_U=0.8)(bnorm)\n",
    "# apply backwards LSTM\n",
    "backwards = GRU(lstm_neurons, dropout_W=0.8, dropout_U=0.8, go_backwards=True)(bnorm)\n",
    "\n",
    "# concatenate the outputs of the 2 LSTMs\n",
    "merged = merge([forwards, backwards], mode='concat', concat_axis=-1)\n",
    "after_dp = Dropout(0.8)(merged)\n",
    "output = Dense(1, activation='sigmoid')(after_dp)\n",
    "\n",
    "model_bidir_google = Model(input=sequence, output=output)\n",
    "# review model structure\n",
    "print(model_bidir_google.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train...\n",
      "Train on 2780 samples, validate on 1390 samples\n",
      "Epoch 1/16\n",
      "46s - loss: 0.9940 - acc: 0.6129 - val_loss: 0.9379 - val_acc: 0.7201\n",
      "Epoch 2/16\n",
      "38s - loss: 0.9165 - acc: 0.6198 - val_loss: 0.8582 - val_acc: 0.7201\n",
      "Epoch 3/16\n",
      "48s - loss: 0.8347 - acc: 0.6266 - val_loss: 0.8077 - val_acc: 0.7201\n",
      "Epoch 4/16\n",
      "53s - loss: 0.7608 - acc: 0.6302 - val_loss: 0.7630 - val_acc: 0.7201\n",
      "Epoch 5/16\n",
      "52s - loss: 0.7035 - acc: 0.6601 - val_loss: 0.6987 - val_acc: 0.7187\n",
      "Epoch 6/16\n",
      "46s - loss: 0.6682 - acc: 0.6691 - val_loss: 0.6709 - val_acc: 0.7194\n",
      "Epoch 7/16\n",
      "60s - loss: 0.6360 - acc: 0.6863 - val_loss: 0.6642 - val_acc: 0.7187\n",
      "Epoch 8/16\n",
      "43s - loss: 0.6258 - acc: 0.6978 - val_loss: 0.6430 - val_acc: 0.7187\n",
      "Epoch 9/16\n",
      "15s - loss: 0.6136 - acc: 0.7119 - val_loss: 0.6155 - val_acc: 0.7187\n",
      "Epoch 10/16\n",
      "18s - loss: 0.6087 - acc: 0.7162 - val_loss: 0.6389 - val_acc: 0.7187\n",
      "Epoch 11/16\n",
      "49s - loss: 0.6093 - acc: 0.7183 - val_loss: 0.6067 - val_acc: 0.7187\n",
      "Epoch 12/16\n",
      "41s - loss: 0.6060 - acc: 0.7173 - val_loss: 0.6031 - val_acc: 0.7187\n",
      "Epoch 13/16\n",
      "30s - loss: 0.6021 - acc: 0.7176 - val_loss: 0.6003 - val_acc: 0.7187\n",
      "Epoch 14/16\n",
      "43s - loss: 0.6046 - acc: 0.7194 - val_loss: 0.5969 - val_acc: 0.7187\n",
      "Epoch 15/16\n",
      "13s - loss: 0.6034 - acc: 0.7180 - val_loss: 0.6061 - val_acc: 0.7187\n",
      "Epoch 16/16\n",
      "29s - loss: 0.6040 - acc: 0.7183 - val_loss: 0.6032 - val_acc: 0.7187\n",
      "avg sec per epoch: 41.2291875035\n"
     ]
    }
   ],
   "source": [
    "# Bi-directional google\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "model_bidir_google.compile('rmsprop', 'binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "print('Train...')\n",
    "start_time = time.time()\n",
    "\n",
    "\n",
    "history_bidir_google = model_bidir_google.fit(X_train, y_train,\n",
    "                    batch_size=batch_size,\n",
    "                    nb_epoch=epochs,\n",
    "                    validation_data=[X_test, y_test], \n",
    "                    verbose=2)\n",
    "\n",
    "end_time = time.time()\n",
    "average_time_per_epoch = (end_time - start_time) / epochs\n",
    "print(\"avg sec per epoch:\", average_time_per_epoch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
