{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n"
     ]
    }
   ],
   "source": [
    "##Khai báo các thư viện\n",
    "from __future__ import print_function\n",
    "import time\n",
    "import numpy as np\n",
    "np.random.seed(1337)  # for reproducibility\n",
    "\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers import Dense, Dropout, Embedding, LSTM, Input, merge, BatchNormalization,GRU\n",
    "from keras.datasets import imdb\n",
    "\n",
    "import os\n",
    "from keras.preprocessing.text import Tokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Gán nhãn cho dữ liệu train\n",
    "X_train = []\n",
    "y_train = []\n",
    "\n",
    "path = 'C:/KHOALUAN/DATA/individual_stocs_reuter/google/train/pos'\n",
    "X_train.extend([open(path + f).read() for f in os.listdir(path) if f.endswith('.txt')])\n",
    "\n",
    "y_train.extend([1 for _ in range(1384)])\n",
    "\n",
    "path = 'C:/KHOALUAN/DATA/individual_stocs_reuter/google/train/neg/'\n",
    "X_train.extend([open(path + f).read() for f in os.listdir(path) if f.endswith('.txt')])\n",
    "\n",
    "y_train.extend([0 for _ in range(868)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##Gán nhãn cho dữ liệu test\n",
    "X_test = []\n",
    "y_test = []\n",
    "\n",
    "path = 'C:/KHOALUAN/DATA/individual_stocs_reuter/google/test/pos'\n",
    "X_test.extend([open(path + f).read() for f in os.listdir(path) if f.endswith('.txt')])\n",
    "y_test.extend([1 for _ in range(691)])\n",
    "\n",
    "path = 'C:/KHOALUAN/DATA/individual_stocs_reuter/google/test/neg'\n",
    "X_test.extend([open(path + f).read() for f in os.listdir(path) if f.endswith('.txt')])\n",
    "y_test.extend([0 for _ in range(433)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Khai báo tham số đặc trưng và chiều dài câu\n",
    "max_features =900\n",
    "max_len = 5000  # cut texts after this number of words (among top max_features most common words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Xử lý remove stop words cho dữ liệu train\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "X_train_rm= []\n",
    "stop_words=set(stopwords.words(\"vietnamese\"))\n",
    "for x in X_train:\n",
    "    words=word_tokenize(x)\n",
    "    remove_sw= [w for w in words if not unicode(w,\"utf8\") in stop_words]\n",
    "    X_train_rm.append(remove_sw),\n",
    "sentence_train=[] \n",
    "for i in range(len(X_train_rm)):\n",
    "    s = \"\"\n",
    "    for j in range(len(X_train_rm[i])):\n",
    "        s+=X_train_rm[i][j]+\" \"\n",
    "    sentence_train.append(s),\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Xử lý remove stop words cho dữ liệu test\n",
    "X_test_rm= []\n",
    "stop_words=set(stopwords.words(\"vietnamese\"))\n",
    "for x in X_test:\n",
    "    words=word_tokenize(x)\n",
    "    remove_sw= [w for w in words if not unicode(w,\"utf8\") in stop_words]\n",
    "\n",
    "    X_test_rm.append(remove_sw),\n",
    "sentence_test=[] \n",
    "for i in range(len(X_test_rm)):\n",
    "    s = \"\"\n",
    "    for j in range(len(X_test_rm[i])):\n",
    "        s+=X_test_rm[i][j]+\" \"\n",
    "    sentence_test.append(s),\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words: \n",
      "899\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of words: \")\n",
    "print(len(np.unique(np.hstack(X_train ))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review length: \n",
      "Mean 6096.90 words (4040.412157)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYcAAAD8CAYAAACcjGjIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0xe255dd8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot\n",
    "# Summarize review length\n",
    "print(\"Review length: \")\n",
    "result = map(len, X_train)\n",
    "print(\"Mean %.2f words (%f)\" % (np.mean(result), np.std(result)))\n",
    "# plot review length\n",
    "pyplot.boxplot(result)\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Xứ lý tách từ \n",
    "imdbTokenizer = Tokenizer(nb_words=max_features)\n",
    "\n",
    "imdbTokenizer.fit_on_texts(sentence_train)\n",
    "#for word, value in imdbTokenizer.word_index.items():\n",
    "    #print(word),\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "đồng\n",
      "năm\n",
      "vốn\n"
     ]
    }
   ],
   "source": [
    "#create int to word dictionary\n",
    "intToWord = {}\n",
    "for word, value in imdbTokenizer.word_index.items():\n",
    "    intToWord[value] = word\n",
    "\n",
    "#add a symbol for null placeholder\n",
    "intToWord[0] = \"!!!NA!!!\"\n",
    "    \n",
    "print(intToWord[1])\n",
    "print(intToWord[2])\n",
    "print(intToWord[32])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#convert word strings to integer sequence lists\n",
    "#print(X_train[0])\n",
    "#print(imdbTokenizer.texts_to_sequences(X_train[:1]))\n",
    "#for value in imdbTokenizer.texts_to_sequences(X_train[:1])[0]:\n",
    "    #print(intToWord[value])\n",
    "    \n",
    "X_train = imdbTokenizer.texts_to_sequences(sentence_train)\n",
    "X_test = imdbTokenizer.texts_to_sequences(sentence_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "901 train sequences\n",
      "386 test sequences\n",
      "Pad sequences (samples x time)\n",
      "X_train shape: (901L, 5000L)\n",
      "X_test shape: (386L, 5000L)\n"
     ]
    }
   ],
   "source": [
    "print(len(X_train), 'train sequences')\n",
    "print(len(X_test), 'test sequences')\n",
    "\n",
    "print(\"Pad sequences (samples x time)\")\n",
    "X_train = sequence.pad_sequences(X_train, maxlen=max_len)\n",
    "X_test = sequence.pad_sequences(X_test, maxlen=max_len)\n",
    "print('X_train shape:', X_train.shape)\n",
    "print('X_test shape:', X_test.shape)\n",
    "y_train = np.array(y_train)\n",
    "y_test = np.array(y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "epochs = 10\n",
    "embedding_neurons = 128\n",
    "lstm_neurons = 64\n",
    "batch_size =32\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "input_1 (InputLayer)             (None, 5000)          0                                            \n",
      "____________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)          (None, 5000, 128)     115200      input_1[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "batchnormalization_1 (BatchNorma (None, 5000, 128)     512         embedding_1[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "gru_1 (GRU)                      (None, 64)            37056       batchnormalization_1[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)              (None, 64)            0           gru_1[0][0]                      \n",
      "____________________________________________________________________________________________________\n",
      "dense_1 (Dense)                  (None, 1)             65          dropout_1[0][0]                  \n",
      "====================================================================================================\n",
      "Total params: 152,833\n",
      "Trainable params: 152,577\n",
      "Non-trainable params: 256\n",
      "____________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Forward Pass LSTM Network\n",
    "\n",
    "# this is the placeholder tensor for the input sequences\n",
    "sequence = Input(shape=(max_len,), dtype='int32')\n",
    "# this embedding layer will transform the sequences of integers\n",
    "# into vectors of size embedding\n",
    "# embedding layer converts dense int input to one-hot in real time to save memory\n",
    "embedded = Embedding(max_features, embedding_neurons, input_length=max_len)(sequence)\n",
    "# normalize embeddings by input/word in sentence\n",
    "bnorm = BatchNormalization()(embedded)\n",
    "\n",
    "# apply forwards LSTM layer size lstm_neurons\n",
    "forwards = GRU(lstm_neurons, dropout_W=0.4, dropout_U=0.4)(bnorm)\n",
    "\n",
    "# dropout \n",
    "after_dp = Dropout(0.5)(forwards)\n",
    "output = Dense(1, activation='sigmoid')(after_dp)\n",
    "\n",
    "model_fdir_atom = Model(input=sequence, output=output)\n",
    "# review model structure\n",
    "print(model_fdir_atom.summary())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train...\n",
      "Train on 901 samples, validate on 386 samples\n",
      "Epoch 1/10\n",
      "272s - loss: 0.6744 - acc: 0.6681 - precision: 0.7387 - recall: 0.8548 - fmeasure: 0.7839 - val_loss: 0.6749 - val_acc: 0.7591 - val_precision: 0.7720 - val_recall: 0.8109 - val_fmeasure: 0.7764\n",
      "Epoch 2/10\n"
     ]
    }
   ],
   "source": [
    "# Forward pass LSTM network\n",
    "\n",
    "# try using different optimizers and different optimizer configs\n",
    "model_fdir_atom.compile('adam', 'binary_crossentropy', metrics=['accuracy','precision', 'recall', 'fmeasure'])\n",
    "print('Train...')\n",
    "start_time = time.time()\n",
    "\n",
    "history_fdir_atom = model_fdir_atom.fit(X_train, y_train,\n",
    "                    batch_size=batch_size,\n",
    "                    nb_epoch=epochs,\n",
    "                    validation_data=[X_test, y_test], \n",
    "                    verbose=2)\n",
    "\n",
    "end_time = time.time()\n",
    "average_time_per_epoch = (end_time - start_time) / epochs\n",
    "print(\"avg sec per epoch:\", average_time_per_epoch)\n",
    "\n",
    "scores = model_fdir_atom.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "print(history_fdir_atom.history.keys())\n",
    "# summarize history for accuracy\n",
    "plt.plot(history_fdir_atom.history['acc'])\n",
    "plt.plot(history_fdir_atom.history['val_acc'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# summarize history for loss\n",
    "plt.plot(history_fdir_atom.history['loss'])\n",
    "plt.plot(history_fdir_atom.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Bi-directional Atom\n",
    "\n",
    "# based on keras tutorial: https://github.com/fchollet/keras/blob/master/examples/imdb_bidirectional_lstm.py\n",
    "\n",
    "# this is the placeholder tensor for the input sequences\n",
    "sequence = Input(shape=(max_len,), dtype='int32')\n",
    "# this embedding layer will transform the sequences of integers\n",
    "# into vectors of size embedding\n",
    "# embedding layer converts dense int input to one-hot in real time to save memory\n",
    "embedded = Embedding(max_features, embedding_neurons, input_length=max_len)(sequence)\n",
    "# normalize embeddings by input/word in sentence\n",
    "bnorm = BatchNormalization()(embedded)\n",
    "\n",
    "# apply forwards LSTM layer size lstm_neurons\n",
    "forwards = GRU(lstm_neurons, dropout_W=0.4, dropout_U=0.4)(bnorm)\n",
    "# apply backwards LSTM\n",
    "backwards = GRU(lstm_neurons, dropout_W=0.4, dropout_U=0.4, go_backwards=True)(bnorm)\n",
    "\n",
    "# concatenate the outputs of the 2 LSTMs\n",
    "merged = merge([forwards, backwards], mode='concat', concat_axis=-1)\n",
    "after_dp = Dropout(0.5)(merged)\n",
    "output = Dense(1, activation='sigmoid')(after_dp)\n",
    "\n",
    "model_bidir_atom = Model(input=sequence, output=output)\n",
    "# review model structure\n",
    "print(model_bidir_atom.summary())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Bi-directional Atom\n",
    "\n",
    "# try using different optimizers and different optimizer configs\n",
    "model_bidir_atom.compile('rmsprop', 'binary_crossentropy', metrics=['accuracy','precision', 'recall', 'fmeasure'])\n",
    "\n",
    "print('Train...')\n",
    "start_time = time.time()\n",
    "\n",
    "history_bidir_atom = model_bidir_atom.fit(X_train, y_train,\n",
    "                    batch_size=batch_size,\n",
    "                    nb_epoch=epochs,\n",
    "                    validation_data=[X_test, y_test], \n",
    "                    verbose=2)\n",
    "\n",
    "end_time = time.time()\n",
    "average_time_per_epoch = (end_time - start_time) / epochs\n",
    "print(\"avg sec per epoch:\", average_time_per_epoch)\n",
    "scores = model_fdir_atom.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "print(history_fdir_atom.history.keys())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# summarize history for accuracy\n",
    "plt.plot(history_fdir_atom.history['acc'])\n",
    "plt.plot(history_fdir_atom.history['val_acc'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()\n",
    "# summarize history for loss\n",
    "plt.plot(history_fdir_atom.history['loss'])\n",
    "plt.plot(history_fdir_atom.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "print(history_bidir_atom.history.keys())\n",
    "plt.plot(history_bidir_atom.history['acc'])\n",
    "plt.plot(history_bidir_atom.history['val_acc'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
