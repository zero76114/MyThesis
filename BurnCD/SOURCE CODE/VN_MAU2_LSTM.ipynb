{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n"
     ]
    }
   ],
   "source": [
    "##Khai báo các thư viện\n",
    "from __future__ import print_function\n",
    "import time\n",
    "import numpy as np\n",
    "np.random.seed(1337)  # for reproducibility\n",
    "\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers import Dense, Dropout, Embedding, LSTM, Input, merge, BatchNormalization,GRU\n",
    "from keras.datasets import imdb\n",
    "\n",
    "import os\n",
    "from keras.preprocessing.text import Tokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Gán nhãn cho dữ liệu train\n",
    "X_train = []\n",
    "y_train = []\n",
    "\n",
    "path = 'C:/VS_8D/out/train/pos/'\n",
    "X_train.extend([open(path + f).read() for f in os.listdir(path) if f.endswith('.txt')])\n",
    "\n",
    "y_train.extend([1 for _ in range(735)])\n",
    "\n",
    "path = 'C:/VS_8D/out/train/neg/'\n",
    "X_train.extend([open(path + f).read() for f in os.listdir(path) if f.endswith('.txt')])\n",
    "\n",
    "y_train.extend([0 for _ in range(333)])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##Gán nhãn cho dữ liệu test\n",
    "X_test = []\n",
    "y_test = []\n",
    "\n",
    "path = 'C:/VS_8D/out/test/pos/'\n",
    "X_test.extend([open(path + f).read() for f in os.listdir(path) if f.endswith('.txt')])\n",
    "y_test.extend([1 for _ in range(338)])\n",
    "\n",
    "path = 'C:/VS_8D/out/test/neg/'\n",
    "X_test.extend([open(path + f).read() for f in os.listdir(path) if f.endswith('.txt')])\n",
    "y_test.extend([0 for _ in range(119)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Xử lý remove stop words cho dữ liệu train\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "X_train_rm= []\n",
    "stop_words=set(stopwords.words(\"vietnamese\"))\n",
    "for x in X_train:\n",
    "    words=word_tokenize(x)\n",
    "    remove_sw= [w for w in words if not unicode(w,\"utf8\") in stop_words]\n",
    "    X_train_rm.append(remove_sw),\n",
    "sentence_train=[] \n",
    "for i in range(len(X_train_rm)):\n",
    "    s = \"\"\n",
    "    for j in range(len(X_train_rm[i])):\n",
    "        s+=X_train_rm[i][j]+\" \"\n",
    "    sentence_train.append(s),\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Xử lý remove stop words cho dữ liệu test\n",
    "X_test_rm= []\n",
    "stop_words=set(stopwords.words(\"vietnamese\"))\n",
    "for x in X_test:\n",
    "    words=word_tokenize(x)\n",
    "    remove_sw= [w for w in words if not unicode(w,\"utf8\") in stop_words]\n",
    "\n",
    "    X_test_rm.append(remove_sw),\n",
    "sentence_test=[] \n",
    "for i in range(len(X_test_rm)):\n",
    "    s = \"\"\n",
    "    for j in range(len(X_test_rm[i])):\n",
    "        s+=X_test_rm[i][j]+\" \"\n",
    "    sentence_test.append(s),\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Khai báo tham số đặc trưng và chiều dài câu\n",
    "max_features =900\n",
    "max_len = 200  # cut texts after this number of words (among top max_features most common words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Xứ lý tách từ \n",
    "imdbTokenizer = Tokenizer(nb_words=max_features)\n",
    "\n",
    "imdbTokenizer.fit_on_texts(sentence_train)\n",
    "#for word, value in imdbTokenizer.word_index.items():\n",
    "    #print(word),\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "đồng\n",
      "năm\n",
      "án\n"
     ]
    }
   ],
   "source": [
    "#create int to word dictionary\n",
    "intToWord = {}\n",
    "for word, value in imdbTokenizer.word_index.items():\n",
    "    intToWord[value] = word\n",
    "\n",
    "#add a symbol for null placeholder\n",
    "intToWord[0] = \"!!!NA!!!\"\n",
    "    \n",
    "print(intToWord[1])\n",
    "print(intToWord[2])\n",
    "print(intToWord[32])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#convert word strings to integer sequence lists\n",
    "#print(X_train[0])\n",
    "#print(imdbTokenizer.texts_to_sequences(X_train[:1]))\n",
    "#for value in imdbTokenizer.texts_to_sequences(X_train[:1])[0]:\n",
    "    #print(intToWord[value])\n",
    "    \n",
    "X_train = imdbTokenizer.texts_to_sequences(sentence_train)\n",
    "X_test = imdbTokenizer.texts_to_sequences(sentence_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1068 train sequences\n",
      "457 test sequences\n",
      "Pad sequences (samples x time)\n",
      "X_train shape: (1068L, 200L)\n",
      "X_test shape: (457L, 200L)\n"
     ]
    }
   ],
   "source": [
    "print(len(X_train), 'train sequences')\n",
    "print(len(X_test), 'test sequences')\n",
    "\n",
    "print(\"Pad sequences (samples x time)\")\n",
    "X_train = sequence.pad_sequences(X_train, maxlen=max_len)\n",
    "X_test = sequence.pad_sequences(X_test, maxlen=max_len)\n",
    "print('X_train shape:', X_train.shape)\n",
    "print('X_test shape:', X_test.shape)\n",
    "y_train = np.array(y_train)\n",
    "y_test = np.array(y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "epochs = 3\n",
    "embedding_neurons = 64\n",
    "lstm_neurons = 128\n",
    "batch_size =32\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "input_1 (InputLayer)             (None, 200)           0                                            \n",
      "____________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)          (None, 200, 64)       57600       input_1[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "batchnormalization_1 (BatchNorma (None, 200, 64)       256         embedding_1[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                    (None, 128)           98816       batchnormalization_1[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)              (None, 128)           0           lstm_1[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "dense_1 (Dense)                  (None, 1)             129         dropout_1[0][0]                  \n",
      "====================================================================================================\n",
      "Total params: 156,801\n",
      "Trainable params: 156,673\n",
      "Non-trainable params: 128\n",
      "____________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Forward Pass GRU Network\n",
    "\n",
    "# this is the placeholder tensor for the input sequences\n",
    "sequence = Input(shape=(max_len,), dtype='int32')\n",
    "# this embedding layer will transform the sequences of integers\n",
    "# into vectors of size embedding\n",
    "# embedding layer converts dense int input to one-hot in real time to save memory\n",
    "embedded = Embedding(max_features, embedding_neurons, input_length=max_len)(sequence)\n",
    "# normalize embeddings by input/word in sentence\n",
    "bnorm = BatchNormalization()(embedded)\n",
    "\n",
    "# apply forwards GRU layer size lstm_neurons\n",
    "forwards = LSTM(lstm_neurons, dropout_W=0.4, dropout_U=0.4)(bnorm)\n",
    "\n",
    "# dropout \n",
    "after_dp = Dropout(0.5)(forwards)\n",
    "output = Dense(1, activation='sigmoid')(after_dp)\n",
    "\n",
    "model_fdir_atom = Model(input=sequence, output=output)\n",
    "# review model structure\n",
    "print(model_fdir_atom.summary())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train...\n",
      "Train on 1068 samples, validate on 457 samples\n",
      "Epoch 1/3\n",
      "13s - loss: 0.6587 - acc: 0.6330 - precision: 0.6964 - recall: 0.8312 - fmeasure: 0.7515 - val_loss: 0.5895 - val_acc: 0.6942 - val_precision: 0.7396 - val_recall: 0.7702 - val_fmeasure: 0.7506\n",
      "Epoch 2/3\n",
      "13s - loss: 0.6014 - acc: 0.6948 - precision: 0.7054 - recall: 0.9521 - fmeasure: 0.8055 - val_loss: 0.5911 - val_acc: 0.6838 - val_precision: 0.7396 - val_recall: 0.7702 - val_fmeasure: 0.7506\n",
      "Epoch 3/3\n",
      "13s - loss: 0.5674 - acc: 0.7172 - precision: 0.7244 - recall: 0.9510 - fmeasure: 0.8197 - val_loss: 0.5996 - val_acc: 0.7050 - val_precision: 0.7396 - val_recall: 0.7702 - val_fmeasure: 0.7506\n",
      "avg sec per epoch: 17.6966667175\n",
      "Accuracy: 70.50%\n"
     ]
    }
   ],
   "source": [
    "# Bi-directional Atom\n",
    "\n",
    "# try using different optimizers and different optimizer configs\n",
    "model_fdir_atom.compile('rmsprop', 'binary_crossentropy', metrics=['accuracy','precision', 'recall', 'fmeasure'])\n",
    "\n",
    "print('Train...')\n",
    "start_time = time.time()\n",
    "\n",
    "history_bidir_atom = model_fdir_atom.fit(X_train, y_train,\n",
    "                    batch_size=batch_size,\n",
    "                    nb_epoch=epochs,\n",
    "                    validation_data=[X_test, y_test], \n",
    "                    verbose=2)\n",
    "\n",
    "end_time = time.time()\n",
    "average_time_per_epoch = (end_time - start_time) / epochs\n",
    "print(\"avg sec per epoch:\", average_time_per_epoch)\n",
    "scores = model_fdir_atom.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
