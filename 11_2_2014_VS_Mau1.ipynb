{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import time\n",
    "import numpy as np\n",
    "np.random.seed(1337)  # for reproducibility\n",
    "\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers import Dense, Dropout, Embedding, LSTM, Input, merge, BatchNormalization,GRU\n",
    "from keras.datasets import imdb\n",
    "\n",
    "import os\n",
    "from keras.preprocessing.text import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "max_features =899\n",
    "max_len = 500  # cut texts after this number of words (among top max_features most common words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train = []\n",
    "y_train = []\n",
    "\n",
    "path = 'C:/VS_4D/out/train/pos/'\n",
    "X_train.extend([open(path + f).read() for f in os.listdir(path) if f.endswith('.txt')])\n",
    "y_train.extend([1 for _ in range(663)])\n",
    "\n",
    "path = 'C:/VS_4D/out/train/neg/'\n",
    "X_train.extend([open(path + f).read() for f in os.listdir(path) if f.endswith('.txt')])\n",
    "y_train.extend([0 for _ in range(238)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_test = []\n",
    "y_test = []\n",
    "\n",
    "path = 'C:/VS_4D/out/test/pos/'\n",
    "X_test.extend([open(path + f).read() for f in os.listdir(path) if f.endswith('.txt')])\n",
    "y_test.extend([1 for _ in range(298)])\n",
    "\n",
    "path = 'C:/VS_4D/out/test/neg/'\n",
    "X_test.extend([open(path + f).read() for f in os.listdir(path) if f.endswith('.txt')])\n",
    "y_test.extend([0 for _ in range(88)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words: \n",
      "899\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of words: \")\n",
    "print(len(np.unique(np.hstack(X_train ))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review length: \n",
      "Mean 6096.90 words (4040.412157)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYcAAAD8CAYAAACcjGjIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0xa587c18>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot\n",
    "# Summarize review length\n",
    "print(\"Review length: \")\n",
    "result = map(len, X_train)\n",
    "print(\"Mean %.2f words (%f)\" % (np.mean(result), np.std(result)))\n",
    "# plot review length\n",
    "pyplot.boxplot(result)\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "#tokenize works to list of integers where each integer is a key to a word\n",
    "imdbTokenizer = Tokenizer(nb_words=max_features)\n",
    "\n",
    "imdbTokenizer.fit_on_texts(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "và\n",
      "trong\n",
      "đó\n"
     ]
    }
   ],
   "source": [
    "#create int to word dictionary\n",
    "intToWord = {}\n",
    "for word, value in imdbTokenizer.word_index.items():\n",
    "    intToWord[value] = word\n",
    "\n",
    "#add a symbol for null placeholder\n",
    "intToWord[0] = \"!!!NA!!!\"\n",
    "    \n",
    "print(intToWord[1])\n",
    "print(intToWord[2])\n",
    "print(intToWord[32])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[344 353  54  67 161  17  67 308  15   5   3  30  78 124 207   5   3 245\n",
      "   8 161  22 271 101  62  97 219 491 234 156  37  75 156 231 447 794 242\n",
      "  78 344 353  54  67 161   3  14  17  67 308  15   5   3  30  78 124 207\n",
      "   5   3 245   8 161  22 271 101  62  97 219 491 234 156 206 354 156  96\n",
      "  78 344 353  54  67 161  17  67 308  15   5  76   3  30  78 124 207   5\n",
      "   3 245   8 161  22 271 101  62  97 219 491 234 245  76 218 156 195 228\n",
      "   1 209  26  37 156 195 228  78 344 353  54  67 161   3  14  17  67 308\n",
      "  15   5  76   3  30  78 124 207   5   3 245   8 161  22 271 101  62  97\n",
      " 219 491 234 156 424 517   1 538 156 424 203 285 202 123 363 247  78 344\n",
      " 353  54  67 161  17  67 308  15   5   3  30  78 124 207   5   3 161  22\n",
      " 271 101  62  97 219 491 234 156  37  75 156 583 472   1   7  37 242  24\n",
      " 788 583 472  78 344 353  54  67 161   3  14  17  67 308  15   5   3  30\n",
      "  78 124 207   5   3 245   8 161  22 271 101  62  97 219 491 234 156 220\n",
      " 788 156 220 788 194  78 344 353  54  67 161  17  67 308  15   5   3  30\n",
      "  78 124 207   5   3 161  22 271 101  62  97 219 491 234 156 195 228   1\n",
      " 209  26  37 156 195 228  78 344 353  54  67 161   3  14  17  67 308  15\n",
      "   5   3  30  78 124 207   5   3 245   8 161  22 271 101  62  97 219 491\n",
      " 234 156  37  75 156 100 242  78 344 353  54  67 161   3  14  17  67 308\n",
      "  15   5   3  30  78 124 207   5   3 245   8 161  22 271 101  62  97 219\n",
      " 491 234 156  37  75 156 231 447 794 242  78 344 353  54  67 161  17  67\n",
      " 308  15   5   3  30  78 124 207   5   3 245   8 161  22 271 101  62  97\n",
      " 219 491 234 156  37  75 156 583 472   1   7  37 242  24 788 583 472  78\n",
      " 344 353  54  67 161  17  67 308  15   5   3  30  78 124 207   5   3 245\n",
      "   8 161  22 271 101  62  97 219 491 234 156  37  75 156 231 447 794 242\n",
      "  78 344 353  54  67 161   3  14  17  67 308  15   5   3  30  78 124 207\n",
      "   5   3 245   8 161  22 271 101  62  97 219 491 234 156 206 354 156  96\n",
      "  78 344 353  54  67 161  17  67 308  15   5  76   3  30  78 124 207   5\n",
      "   3 245   8 161  22 271 101  62  97 219 491 234 245  76]\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'lower'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-76e8393caad4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[1;31m#convert word strings to integer sequence lists\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[1;32mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[1;32mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimdbTokenizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtexts_to_sequences\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mvalue\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mimdbTokenizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtexts_to_sequences\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[1;32mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mintToWord\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\HaNa\\Anaconda2\\lib\\site-packages\\keras\\preprocessing\\text.pyc\u001b[0m in \u001b[0;36mtexts_to_sequences\u001b[0;34m(self, texts)\u001b[0m\n\u001b[1;32m    162\u001b[0m         \"\"\"\n\u001b[1;32m    163\u001b[0m         \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 164\u001b[0;31m         \u001b[1;32mfor\u001b[0m \u001b[0mvect\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtexts_to_sequences_generator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtexts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    165\u001b[0m             \u001b[0mres\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvect\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mres\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\HaNa\\Anaconda2\\lib\\site-packages\\keras\\preprocessing\\text.pyc\u001b[0m in \u001b[0;36mtexts_to_sequences_generator\u001b[0;34m(self, texts)\u001b[0m\n\u001b[1;32m    183\u001b[0m                                                                      \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfilters\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m                                                                      \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 185\u001b[0;31m                                                                      self.split)\n\u001b[0m\u001b[1;32m    186\u001b[0m             \u001b[0mvect\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mseq\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\HaNa\\Anaconda2\\lib\\site-packages\\keras\\preprocessing\\text.pyc\u001b[0m in \u001b[0;36mtext_to_word_sequence\u001b[0;34m(text, filters, lower, split)\u001b[0m\n\u001b[1;32m     34\u001b[0m     \"\"\"\n\u001b[1;32m     35\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mlower\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m         \u001b[0mtext\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m     \u001b[0mtext\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtranslate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmaketrans\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msplit\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilters\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0mseq\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'lower'"
     ]
    }
   ],
   "source": [
    "#convert word strings to integer sequence lists\n",
    "#print(X_train[0])\n",
    "#print(imdbTokenizer.texts_to_sequences(X_train[:1]))\n",
    "#for value in imdbTokenizer.texts_to_sequences(X_train[:1])[0]:\n",
    "    #print(intToWord[value])\n",
    "    \n",
    "X_train = imdbTokenizer.texts_to_sequences(X_train)\n",
    "X_test = imdbTokenizer.texts_to_sequences(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "901 train sequences\n",
      "386 test sequences\n",
      "Pad sequences (samples x time)\n",
      "X_train shape: (901L, 500L)\n",
      "X_test shape: (386L, 500L)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Censor the data by having a max review length (in number of words)\n",
    "\n",
    "#use this function to load data from keras pickle instead of munging as shown above\n",
    "#(X_train, y_train), (X_test, y_test) = imdb.load_data(nb_words=max_features,\n",
    "#                                                      test_split=0.2)\n",
    "\n",
    "print(len(X_train), 'train sequences')\n",
    "print(len(X_test), 'test sequences')\n",
    "\n",
    "print(\"Pad sequences (samples x time)\")\n",
    "X_train = sequence.pad_sequences(X_train, maxlen=max_len)\n",
    "X_test = sequence.pad_sequences(X_test, maxlen=max_len)\n",
    "print('X_train shape:', X_train.shape)\n",
    "print('X_test shape:', X_test.shape)\n",
    "y_train = np.array(y_train)\n",
    "y_test = np.array(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1]\n"
     ]
    }
   ],
   "source": [
    "print(np.unique(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "epochs = 3\n",
    "embedding_neurons = 32\n",
    "lstm_neurons = 128\n",
    "batch_size =32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "input_1 (InputLayer)             (None, 500)           0                                            \n",
      "____________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)          (None, 500, 32)       28768       input_1[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "batchnormalization_1 (BatchNorma (None, 500, 32)       128         embedding_1[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "gru_1 (GRU)                      (None, 128)           61824       batchnormalization_1[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)              (None, 128)           0           gru_1[0][0]                      \n",
      "____________________________________________________________________________________________________\n",
      "dense_1 (Dense)                  (None, 1)             129         dropout_1[0][0]                  \n",
      "====================================================================================================\n",
      "Total params: 90,849\n",
      "Trainable params: 90,785\n",
      "Non-trainable params: 64\n",
      "____________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Forward Pass LSTM Network\n",
    "\n",
    "# this is the placeholder tensor for the input sequences\n",
    "sequence = Input(shape=(max_len,), dtype='int32')\n",
    "# this embedding layer will transform the sequences of integers\n",
    "# into vectors of size embedding\n",
    "# embedding layer converts dense int input to one-hot in real time to save memory\n",
    "embedded = Embedding(max_features, embedding_neurons, input_length=max_len)(sequence)\n",
    "# normalize embeddings by input/word in sentence\n",
    "bnorm = BatchNormalization()(embedded)\n",
    "\n",
    "# apply forwards LSTM layer size lstm_neurons\n",
    "forwards = GRU(lstm_neurons, dropout_W=0.3, dropout_U=0.3)(bnorm)\n",
    "\n",
    "# dropout \n",
    "after_dp = Dropout(0.4)(forwards)\n",
    "output = Dense(1, activation='sigmoid')(after_dp)\n",
    "\n",
    "model_fdir_atom = Model(input=sequence, output=output)\n",
    "# review model structure\n",
    "print(model_fdir_atom.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train...\n",
      "Train on 901 samples, validate on 386 samples\n",
      "Epoch 1/3\n",
      "87s - loss: 0.6540 - acc: 0.6648 - val_loss: 0.6038 - val_acc: 0.7720\n",
      "Epoch 2/3\n",
      "91s - loss: 0.5759 - acc: 0.7347 - val_loss: 0.5986 - val_acc: 0.7720\n",
      "Epoch 3/3\n",
      "86s - loss: 0.5289 - acc: 0.7558 - val_loss: 0.5796 - val_acc: 0.7720\n",
      "avg sec per epoch: 96.9490000407\n",
      "Accuracy: 77.20%\n"
     ]
    }
   ],
   "source": [
    "# Forward pass LSTM network\n",
    "\n",
    "# try using different optimizers and different optimizer configs\n",
    "model_fdir_atom.compile('adam', 'binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "print('Train...')\n",
    "start_time = time.time()\n",
    "\n",
    "history_fdir_atom = model_fdir_atom.fit(X_train, y_train,\n",
    "                    batch_size=batch_size,\n",
    "                    nb_epoch=epochs,\n",
    "                    validation_data=[X_test, y_test], \n",
    "                    verbose=2)\n",
    "\n",
    "end_time = time.time()\n",
    "average_time_per_epoch = (end_time - start_time) / epochs\n",
    "print(\"avg sec per epoch:\", average_time_per_epoch)\n",
    "\n",
    "scores = model_fdir_atom.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "input_2 (InputLayer)             (None, 500)           0                                            \n",
      "____________________________________________________________________________________________________\n",
      "embedding_2 (Embedding)          (None, 500, 32)       28768       input_2[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "batchnormalization_2 (BatchNorma (None, 500, 32)       128         embedding_2[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "gru_2 (GRU)                      (None, 128)           61824       batchnormalization_2[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "gru_3 (GRU)                      (None, 128)           61824       batchnormalization_2[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "merge_1 (Merge)                  (None, 256)           0           gru_2[0][0]                      \n",
      "                                                                   gru_3[0][0]                      \n",
      "____________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)              (None, 256)           0           merge_1[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "dense_2 (Dense)                  (None, 1)             257         dropout_2[0][0]                  \n",
      "====================================================================================================\n",
      "Total params: 152,801\n",
      "Trainable params: 152,737\n",
      "Non-trainable params: 64\n",
      "____________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Bi-directional Atom\n",
    "\n",
    "# based on keras tutorial: https://github.com/fchollet/keras/blob/master/examples/imdb_bidirectional_lstm.py\n",
    "\n",
    "# this is the placeholder tensor for the input sequences\n",
    "sequence = Input(shape=(max_len,), dtype='int32')\n",
    "# this embedding layer will transform the sequences of integers\n",
    "# into vectors of size embedding\n",
    "# embedding layer converts dense int input to one-hot in real time to save memory\n",
    "embedded = Embedding(max_features, embedding_neurons, input_length=max_len)(sequence)\n",
    "# normalize embeddings by input/word in sentence\n",
    "bnorm = BatchNormalization()(embedded)\n",
    "\n",
    "# apply forwards LSTM layer size lstm_neurons\n",
    "forwards = GRU(lstm_neurons, dropout_W=0.3, dropout_U=0.3)(bnorm)\n",
    "# apply backwards LSTM\n",
    "backwards = GRU(lstm_neurons, dropout_W=0.3, dropout_U=0.3, go_backwards=True)(bnorm)\n",
    "\n",
    "# concatenate the outputs of the 2 LSTMs\n",
    "merged = merge([forwards, backwards], mode='concat', concat_axis=-1)\n",
    "after_dp = Dropout(0.4)(merged)\n",
    "output = Dense(1, activation='sigmoid')(after_dp)\n",
    "\n",
    "model_bidir_atom = Model(input=sequence, output=output)\n",
    "# review model structure\n",
    "print(model_bidir_atom.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train...\n",
      "Train on 901 samples, validate on 386 samples\n",
      "Epoch 1/3\n",
      "166s - loss: 0.6435 - acc: 0.6648 - val_loss: 0.6031 - val_acc: 0.7720\n",
      "Epoch 2/3\n",
      "175s - loss: 0.5528 - acc: 0.7381 - val_loss: 0.5842 - val_acc: 0.7720\n",
      "Epoch 3/3\n",
      "185s - loss: 0.5247 - acc: 0.7658 - val_loss: 0.5651 - val_acc: 0.7720\n",
      "avg sec per epoch: 192.251333396\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Bi-directional Atom\n",
    "\n",
    "# try using different optimizers and different optimizer configs\n",
    "model_bidir_atom.compile('rmsprop', 'binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "print('Train...')\n",
    "start_time = time.time()\n",
    "\n",
    "history_bidir_atom = model_bidir_atom.fit(X_train, y_train,\n",
    "                    batch_size=batch_size,\n",
    "                    nb_epoch=epochs,\n",
    "                    validation_data=[X_test, y_test], \n",
    "                    verbose=2)\n",
    "\n",
    "end_time = time.time()\n",
    "average_time_per_epoch = (end_time - start_time) / epochs\n",
    "print(\"avg sec per epoch:\", average_time_per_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test = \"hhbi\"\n",
    "\n",
    "test = imdbTokenizer.texts_to_sequences([test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1L, 500L)\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing import sequence\n",
    "test = sequence.pad_sequences(test, maxlen=max_len)\n",
    "print(test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.6093834]]\n"
     ]
    }
   ],
   "source": [
    "predict=model_bidir_atom.predict(test)\n",
    "print(predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.0]\n"
     ]
    }
   ],
   "source": [
    "rounded = [round(x) for x in predict]\n",
    "print(rounded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
