{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM IMDB Movie Review Tutorial\n",
    "Josiah Olson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import time\n",
    "import numpy as np\n",
    "np.random.seed(1337)  # for reproducibility\n",
    "\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers import Dense, Dropout, Embedding, LSTM, Input, merge, BatchNormalization,GRU\n",
    "from keras.datasets import imdb\n",
    "\n",
    "import os\n",
    "from keras.preprocessing.text import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "max_features =10000\n",
    "max_len = 50  # cut texts after this number of words (among top max_features most common words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x:\n",
      "['\\xef\\xbb\\xbfABI: Ng\\xc3\\xa0y GDKHQ T\\xe1\\xbb\\x95 ch\\xe1\\xbb\\xa9c \\xc4\\x90\\xe1\\xba\\xa1i h\\xe1\\xbb\\x99i \\xc4\\x91\\xe1\\xbb\\x93ng c\\xe1\\xbb\\x95 \\xc4\\x91\\xc3\\xb4ng th\\xc6\\xb0\\xe1\\xbb\\x9dng ni\\xc3\\xaan n\\xc4\\x83m 2014 v\\xc3\\xa0 T\\xe1\\xba\\xa1m \\xe1\\xbb\\xa9ng c\\xe1\\xbb\\x95 t\\xe1\\xbb\\xa9c n\\xc4\\x83m 2013 b\\xe1\\xba\\xb1ng ti\\xe1\\xbb\\x81n (10%)\\n']\n",
      "['\\xef\\xbb\\xbfNPS: Ng\\xc3\\xa0y GDKHQ \\xc4\\x90\\xe1\\xba\\xa1i h\\xe1\\xbb\\x99i c\\xe1\\xbb\\x95 \\xc4\\x91\\xc3\\xb4ng th\\xc6\\xb0\\xe1\\xbb\\x9dng ni\\xc3\\xaan n\\xc4\\x83m 2014\\n']\n",
      "2086\n",
      "y:\n",
      "[1]\n",
      "[0]\n",
      "2086\n"
     ]
    }
   ],
   "source": [
    "# get dataset and unzip: http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
    "\n",
    "X_train = []\n",
    "y_train = []\n",
    "\n",
    "path = 'C:/1_Research/Create_data/aclImdb/train_sukien55/pos/'\n",
    "X_train.extend([open(path + f).read() for f in os.listdir(path) if f.endswith('.txt')])\n",
    "y_train.extend([1 for _ in range(1499)])\n",
    "\n",
    "path = 'C:/1_Research/Create_data/aclImdb/train_sukien55/neg/'\n",
    "X_train.extend([open(path + f).read() for f in os.listdir(path) if f.endswith('.txt')])\n",
    "y_train.extend([0 for _ in range(587)])\n",
    "\n",
    "print('x:')\n",
    "print(X_train[:1])\n",
    "print(X_train[-1:])\n",
    "print(len(X_train))\n",
    "print('y:')\n",
    "print(y_train[:1])\n",
    "print(y_train[-1:])\n",
    "print(len(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x:\n",
      "['\\xef\\xbb\\xbfLCC: Ng\\xc3\\xa0y  GDKHQ T\\xe1\\xbb\\x95 ch\\xe1\\xbb\\xa9c \\xc4\\x90\\xe1\\xba\\xa1i h\\xe1\\xbb\\x99i \\xc4\\x91\\xe1\\xbb\\x93ng c\\xe1\\xbb\\x95 \\xc4\\x91\\xc3\\xb4ng n\\xc4\\x83m 2014\\n']\n",
      "['\\xef\\xbb\\xbfYBC: Ng\\xc3\\xa0y GDKHQ tham d\\xe1\\xbb\\xb1 \\xc4\\x90\\xe1\\xba\\xa1i h\\xe1\\xbb\\x99i \\xc4\\x91\\xe1\\xbb\\x93ng c\\xe1\\xbb\\x95 \\xc4\\x91\\xc3\\xb4ng th\\xc6\\xb0\\xe1\\xbb\\x9dng ni\\xc3\\xaan 2016\\n']\n",
      "2084\n",
      "y:\n",
      "[1]\n",
      "[0]\n",
      "2084\n"
     ]
    }
   ],
   "source": [
    "# read in the test data\n",
    "\n",
    "X_test = []\n",
    "y_test = []\n",
    "\n",
    "path = 'C:/1_Research/Create_data/aclImdb/test_sukien55/pos/'\n",
    "X_test.extend([open(path + f).read() for f in os.listdir(path) if f.endswith('.txt')])\n",
    "y_test.extend([1 for _ in range(1498)])\n",
    "\n",
    "path = 'C:/1_Research/Create_data/aclImdb/test_sukien55/neg/'\n",
    "X_test.extend([open(path + f).read() for f in os.listdir(path) if f.endswith('.txt')])\n",
    "y_test.extend([0 for _ in range(586)])\n",
    "\n",
    "print('x:')\n",
    "print(X_test[:1])\n",
    "print(X_test[-1:])\n",
    "print(len(X_test))\n",
    "print('y:')\n",
    "print(y_test[:1])\n",
    "print(y_test[-1:])\n",
    "print(len(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#tokenize works to list of integers where each integer is a key to a word\n",
    "imdbTokenizer = Tokenizer(nb_words=max_features)\n",
    "\n",
    "imdbTokenizer.fit_on_texts(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 đông\n",
      "3 gdkhq\n",
      "8 Đại\n",
      "4 năm\n",
      "19 dự\n",
      "12 tức\n",
      "15 2015\n",
      "13 2014\n",
      "1 cổ\n",
      "6 thường\n",
      "7 hội\n",
      "2 ngày\n",
      "10 đồng\n",
      "16 chức\n",
      "18 trả\n",
      "17 tổ\n",
      "14 tiền\n",
      "11 bằng\n",
      "9 niên\n"
     ]
    }
   ],
   "source": [
    "#print top 20 words \n",
    "#note zero is reserved for non frequent words\n",
    "for word, value in imdbTokenizer.word_index.items():\n",
    "    if value < 20:\n",
    "        print(value, word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cổ\n",
      "ngày\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "#create int to word dictionary\n",
    "intToWord = {}\n",
    "for word, value in imdbTokenizer.word_index.items():\n",
    "    intToWord[value] = word\n",
    "\n",
    "#add a symbol for null placeholder\n",
    "intToWord[0] = \"!!!NA!!!\"\n",
    "    \n",
    "print(intToWord[1])\n",
    "print(intToWord[2])\n",
    "print(intToWord[32])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "﻿ABI: Ngày GDKHQ Tổ chức Đại hội đồng cổ đông thường niên năm 2014 và Tạm ứng cổ tức năm 2013 bằng tiền (10%)\n",
      "\n",
      "[[578, 2, 3, 17, 16, 8, 7, 10, 1, 5, 6, 9, 4, 13, 27, 24, 23, 1, 12, 4, 26, 11, 14, 32]]\n",
      "﻿abi\n",
      "ngày\n",
      "gdkhq\n",
      "tổ\n",
      "chức\n",
      "Đại\n",
      "hội\n",
      "đồng\n",
      "cổ\n",
      "đông\n",
      "thường\n",
      "niên\n",
      "năm\n",
      "2014\n",
      "và\n",
      "tạm\n",
      "ứng\n",
      "cổ\n",
      "tức\n",
      "năm\n",
      "2013\n",
      "bằng\n",
      "tiền\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "#convert word strings to integer sequence lists\n",
    "print(X_train[0])\n",
    "print(imdbTokenizer.texts_to_sequences(X_train[:1]))\n",
    "for value in imdbTokenizer.texts_to_sequences(X_train[:1])[0]:\n",
    "    print(intToWord[value])\n",
    "    \n",
    "X_train = imdbTokenizer.texts_to_sequences(X_train)\n",
    "X_test = imdbTokenizer.texts_to_sequences(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2086 train sequences\n",
      "2084 test sequences\n",
      "Pad sequences (samples x time)\n",
      "X_train shape: (2086L, 50L)\n",
      "X_test shape: (2084L, 50L)\n"
     ]
    }
   ],
   "source": [
    "# Censor the data by having a max review length (in number of words)\n",
    "\n",
    "#use this function to load data from keras pickle instead of munging as shown above\n",
    "#(X_train, y_train), (X_test, y_test) = imdb.load_data(nb_words=max_features,\n",
    "#                                                      test_split=0.2)\n",
    "\n",
    "print(len(X_train), 'train sequences')\n",
    "print(len(X_test), 'test sequences')\n",
    "\n",
    "print(\"Pad sequences (samples x time)\")\n",
    "X_train = sequence.pad_sequences(X_train, maxlen=max_len)\n",
    "X_test = sequence.pad_sequences(X_test, maxlen=max_len)\n",
    "print('X_train shape:', X_train.shape)\n",
    "print('X_test shape:', X_test.shape)\n",
    "y_train = np.array(y_train)\n",
    "y_test = np.array(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0 578   2   3  17  16   8   7  10   1   5\n",
      "   6   9   4  13  27  24  23   1  12   4  26  11  14  32]\n",
      "y: 1\n"
     ]
    }
   ],
   "source": [
    "#example of a sentence sequence, note that lower integers are words that occur more commonly\n",
    "print(\"x:\", X_train[0]) #per observation vector of 20000 words\n",
    "print(\"y:\", y_train[0]) #positive or negative review encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y distribution: (array([0, 1]), array([ 587, 1499], dtype=int64))\n",
      "max x word: 895 ; min x word 0\n",
      "y distribution test: (array([0, 1]), array([ 586, 1498], dtype=int64))\n",
      "max x word test: 866 ; min x word 0\n"
     ]
    }
   ],
   "source": [
    "# double check that word sequences behave/final dimensions are as expected\n",
    "print(\"y distribution:\", np.unique(y_train, return_counts=True))\n",
    "print(\"max x word:\", np.max(X_train), \"; min x word\", np.min(X_train))\n",
    "print(\"y distribution test:\", np.unique(y_test, return_counts=True))\n",
    "print(\"max x word test:\", np.max(X_test), \"; min x word\", np.min(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "most and least popular words: \n",
      "(array([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,\n",
      "        13,  14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,\n",
      "        26,  27,  28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,\n",
      "        39,  40,  41,  42,  43,  44,  45,  46,  47,  48,  49,  50,  51,\n",
      "        52,  53,  54,  55,  56,  57,  58,  59,  60,  61,  62,  63,  64,\n",
      "        65,  66,  67,  68,  69,  70,  71,  72,  73,  74,  75,  76,  77,\n",
      "        78,  79,  80,  81,  82,  83,  84,  85,  86,  87,  88,  89,  90,\n",
      "        91,  92,  93,  94,  95,  96,  97,  98,  99, 100, 101, 102, 103,\n",
      "       104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116,\n",
      "       117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129,\n",
      "       130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142,\n",
      "       143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155,\n",
      "       156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168,\n",
      "       169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181,\n",
      "       182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194,\n",
      "       195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207,\n",
      "       208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220,\n",
      "       221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233,\n",
      "       234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246,\n",
      "       247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259,\n",
      "       260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272,\n",
      "       273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285,\n",
      "       286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298,\n",
      "       299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311,\n",
      "       312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324,\n",
      "       325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337,\n",
      "       338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350,\n",
      "       351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363,\n",
      "       364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376,\n",
      "       377, 378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389,\n",
      "       390, 391, 392, 393, 394, 395, 396, 397, 398, 399, 400, 401, 402,\n",
      "       403, 404, 405, 406, 407, 408, 409, 410, 411, 412, 413, 414, 415,\n",
      "       416, 417, 418, 419, 420, 421, 422, 423, 424, 425, 426, 427, 428,\n",
      "       429, 430, 431, 432, 433, 434, 435, 436, 437, 438, 439, 440, 441,\n",
      "       442, 443, 444, 445, 446, 447, 448, 449, 450, 451, 452, 453, 454,\n",
      "       455, 456, 457, 458, 459, 460, 461, 462, 463, 464, 465, 466, 467,\n",
      "       468, 469, 470, 471, 472, 473, 474, 475, 476, 477, 478, 479, 480,\n",
      "       481, 482, 483, 484, 485, 486, 487, 488, 489, 490, 491, 492, 493,\n",
      "       494, 495, 496, 497, 498, 499, 500, 501, 502, 503, 504, 505, 506,\n",
      "       507, 508, 509, 510, 511, 512, 513, 514, 515, 516, 517, 518, 519,\n",
      "       520, 521, 522, 523, 524, 525, 526, 527, 528, 529, 530, 531, 532,\n",
      "       533, 534, 535, 536, 537, 538, 539, 540, 541, 542, 543, 544, 545,\n",
      "       546, 547, 548, 549, 550, 551, 552, 553, 554, 555, 556, 557, 558,\n",
      "       559, 560, 561, 562, 563, 564, 565, 566, 567, 568, 569, 570, 571,\n",
      "       572, 573, 574, 575, 576, 577, 578, 579, 580, 581, 582, 583, 584,\n",
      "       585, 586, 587, 588, 589, 590, 591, 592, 593, 594, 595, 596, 597,\n",
      "       598, 599, 600, 601, 602, 603, 604, 605, 606, 607, 608, 609, 610,\n",
      "       611, 612, 613, 614, 615, 616, 617, 618, 619, 620, 621, 622, 623,\n",
      "       624, 625, 626, 627, 628, 629, 630, 631, 632, 633, 634, 635, 636,\n",
      "       637, 638, 639, 640, 641, 642, 643, 645, 646, 647, 648, 649, 650,\n",
      "       651, 652, 653, 654, 655, 656, 657, 658, 659, 660, 661, 662, 663,\n",
      "       664, 665, 666, 667, 668, 669, 670, 671, 672, 673, 674, 675, 676,\n",
      "       677, 678, 679, 680, 681, 682, 683, 684, 685, 686, 687, 688, 689,\n",
      "       690, 691, 692, 693, 694, 695, 696, 697, 698, 699, 700, 701, 702,\n",
      "       703, 704, 705, 706, 707, 708, 709, 710, 711, 712, 713, 714, 715,\n",
      "       716, 717, 718, 719, 720, 721, 722, 723, 724, 725, 726, 727, 728,\n",
      "       729, 730, 731, 732, 733, 734, 735, 736, 737, 738, 739, 740, 741,\n",
      "       742, 743, 744, 745, 746, 747, 748, 749, 750, 751, 752, 753, 754,\n",
      "       755, 756, 757, 758, 759, 760, 761, 762, 763, 764, 765, 766, 767,\n",
      "       768, 769, 770, 771, 772, 773, 774, 775, 776, 777, 778, 779, 780,\n",
      "       781, 782, 783, 784, 785, 786, 787, 788, 789, 790, 791, 792, 793,\n",
      "       794, 795, 796, 797, 798, 799, 800, 801, 802, 803, 804, 805, 806,\n",
      "       807, 808, 809, 810, 811, 812, 813, 814, 815, 816, 817, 818, 819,\n",
      "       820, 821, 822, 823, 824, 825, 826, 827, 828, 829, 830, 831, 832,\n",
      "       833, 834, 835, 836, 837, 838, 839, 840, 841, 842, 843, 844, 845,\n",
      "       846, 847, 848, 849, 850, 851, 852, 853, 854, 855, 856, 857, 858,\n",
      "       859, 860, 861, 862, 863, 864, 865, 866, 867, 868, 869, 870, 871,\n",
      "       872, 873, 874, 875, 876, 877, 878, 879, 880, 881, 882, 883, 884,\n",
      "       885, 886, 887, 888, 889, 890, 891, 892, 893, 894, 895]), array([74440,  2379,  2069,  2054,  1617,  1287,  1269,  1195,  1182,\n",
      "        1176,   961,   858,   850,   739,   713,   677,   664,   662,\n",
      "         503,   472,   471,   423,   373,   325,   323,   295,   274,\n",
      "         237,   213,   209,   201,   163,   149,   116,   115,   107,\n",
      "          98,    87,    85,    68,    68,    66,    66,    66,    64,\n",
      "          64,    61,    61,    59,    58,    56,    53,    48,    48,\n",
      "          39,    38,    36,    34,    34,    32,    31,    31,    30,\n",
      "          28,    27,    27,    26,    26,    26,    25,    25,    24,\n",
      "          24,    21,    21,    20,    20,    20,    18,    17,    13,\n",
      "          13,    12,    12,    11,    11,    11,    11,    10,    10,\n",
      "           9,     9,     9,     9,     9,     9,     9,     9,     9,\n",
      "           9,     9,     9,     9,     9,     9,     9,     8,     8,\n",
      "           8,     8,     8,     8,     8,     8,     8,     8,     8,\n",
      "           8,     8,     7,     7,     7,     7,     7,     7,     6,\n",
      "           7,     7,     7,     7,     7,     7,     7,     7,     7,\n",
      "           7,     7,     7,     7,     7,     7,     7,     7,     7,\n",
      "           7,     7,     7,     7,     7,     7,     7,     7,     7,\n",
      "           7,     7,     6,     6,     6,     6,     6,     6,     6,\n",
      "           6,     6,     6,     6,     6,     6,     6,     6,     6,\n",
      "           6,     6,     6,     6,     6,     6,     6,     6,     6,\n",
      "           6,     6,     6,     6,     6,     6,     6,     6,     6,\n",
      "           6,     6,     6,     6,     6,     6,     6,     6,     6,\n",
      "           6,     6,     6,     6,     6,     6,     6,     6,     6,\n",
      "           6,     6,     6,     6,     6,     6,     6,     6,     6,\n",
      "           6,     6,     6,     6,     6,     6,     6,     6,     6,\n",
      "           6,     6,     5,     5,     5,     5,     5,     5,     5,\n",
      "           5,     5,     5,     5,     5,     5,     5,     5,     5,\n",
      "           5,     5,     5,     5,     5,     5,     5,     5,     5,\n",
      "           5,     5,     5,     5,     5,     5,     5,     5,     5,\n",
      "           5,     5,     5,     5,     5,     5,     5,     5,     5,\n",
      "           5,     5,     5,     5,     5,     5,     5,     5,     5,\n",
      "           5,     5,     5,     5,     5,     5,     5,     5,     5,\n",
      "           5,     5,     5,     5,     5,     5,     5,     5,     5,\n",
      "           5,     5,     5,     5,     5,     5,     5,     5,     5,\n",
      "           5,     5,     5,     5,     5,     5,     5,     5,     5,\n",
      "           4,     4,     4,     4,     4,     4,     4,     4,     4,\n",
      "           4,     4,     4,     4,     4,     4,     4,     4,     4,\n",
      "           4,     4,     4,     4,     4,     4,     4,     4,     4,\n",
      "           4,     4,     4,     4,     4,     4,     4,     4,     4,\n",
      "           4,     4,     4,     4,     4,     4,     4,     4,     4,\n",
      "           4,     4,     4,     4,     4,     4,     4,     4,     4,\n",
      "           4,     4,     4,     4,     4,     4,     4,     4,     4,\n",
      "           4,     4,     4,     4,     4,     4,     4,     4,     4,\n",
      "           4,     4,     4,     4,     4,     4,     4,     4,     4,\n",
      "           4,     4,     4,     4,     4,     4,     4,     4,     4,\n",
      "           4,     4,     3,     3,     3,     3,     3,     3,     3,\n",
      "           3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "           3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "           3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "           3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "           3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "           3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "           3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "           3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "           3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "           3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "           3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "           3,     3,     3,     3,     3,     3,     2,     2,     2,\n",
      "           2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "           2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "           2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "           2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "           2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "           2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "           2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "           2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "           2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "           2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "           2,     1,     2,     2,     2,     2,     1,     1,     1,\n",
      "           1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "           1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "           1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "           1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "           1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "           1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "           1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "           1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "           1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "           1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "           1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "           1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "           1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "           1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "           1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "           1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "           1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "           1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "           1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "           1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "           1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "           1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "           1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "           1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "           1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "           1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "           1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "           1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "           1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "           1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "           1,     1,     1,     1], dtype=int64))\n"
     ]
    }
   ],
   "source": [
    "print(\"most and least popular words: \")\n",
    "print(np.unique(X_train, return_counts=True))\n",
    "# as expected zero is the highly used word for words not in index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#set model hyper parameters\n",
    "epochs = 50\n",
    "embedding_neurons = 500\n",
    "lstm_neurons = 100\n",
    "batch_size =100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "input_13 (InputLayer)            (None, 50)            0                                            \n",
      "____________________________________________________________________________________________________\n",
      "embedding_13 (Embedding)         (None, 50, 500)       5000000     input_13[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "batchnormalization_13 (BatchNorma(None, 50, 500)       1000        embedding_13[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "lstm_10 (LSTM)                   (None, 100)           240400      batchnormalization_13[0][0]      \n",
      "____________________________________________________________________________________________________\n",
      "dropout_13 (Dropout)             (None, 100)           0           lstm_10[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "dense_13 (Dense)                 (None, 1)             101         dropout_13[0][0]                 \n",
      "====================================================================================================\n",
      "Total params: 5241501\n",
      "____________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Forward Pass LSTM Network\n",
    "\n",
    "# this is the placeholder tensor for the input sequences\n",
    "sequence = Input(shape=(max_len,), dtype='int32')\n",
    "# this embedding layer will transform the sequences of integers\n",
    "# into vectors of size embedding\n",
    "# embedding layer converts dense int input to one-hot in real time to save memory\n",
    "embedded = Embedding(max_features, embedding_neurons, input_length=max_len)(sequence)\n",
    "# normalize embeddings by input/word in sentence\n",
    "bnorm = BatchNormalization()(embedded)\n",
    "\n",
    "# apply forwards LSTM layer size lstm_neurons\n",
    "forwards = LSTM(lstm_neurons, dropout_W=0.2, dropout_U=0.2)(bnorm)\n",
    "\n",
    "# dropout \n",
    "after_dp = Dropout(0.5)(forwards)\n",
    "output = Dense(1, activation='sigmoid')(after_dp)\n",
    "\n",
    "model_fdir_atom = Model(input=sequence, output=output)\n",
    "# review model structure\n",
    "print(model_fdir_atom.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train...\n",
      "Train on 2086 samples, validate on 2084 samples\n",
      "Epoch 1/50\n",
      "23s - loss: 0.6537 - acc: 0.6903 - val_loss: 0.5980 - val_acc: 0.7188\n",
      "Epoch 2/50\n",
      "24s - loss: 0.5957 - acc: 0.7215 - val_loss: 0.6082 - val_acc: 0.7145\n",
      "Epoch 3/50\n",
      "23s - loss: 0.5884 - acc: 0.7210 - val_loss: 0.6085 - val_acc: 0.7145\n",
      "Epoch 4/50\n",
      "24s - loss: 0.5883 - acc: 0.7196 - val_loss: 0.6038 - val_acc: 0.7135\n",
      "Epoch 5/50\n",
      "24s - loss: 0.5666 - acc: 0.7315 - val_loss: 0.6113 - val_acc: 0.7087\n",
      "Epoch 6/50\n",
      "24s - loss: 0.5298 - acc: 0.7464 - val_loss: 0.6316 - val_acc: 0.7063\n",
      "Epoch 7/50\n",
      "26s - loss: 0.4881 - acc: 0.7733 - val_loss: 0.6343 - val_acc: 0.7063\n",
      "Epoch 8/50\n",
      "28s - loss: 0.4416 - acc: 0.8054 - val_loss: 0.6700 - val_acc: 0.6166\n",
      "Epoch 9/50\n",
      "24s - loss: 0.3766 - acc: 0.8418 - val_loss: 0.7345 - val_acc: 0.4328\n",
      "Epoch 10/50\n",
      "25s - loss: 0.3181 - acc: 0.8663 - val_loss: 0.7543 - val_acc: 0.4434\n",
      "Epoch 11/50\n",
      "25s - loss: 0.2889 - acc: 0.8792 - val_loss: 0.7389 - val_acc: 0.5547\n",
      "Epoch 12/50\n",
      "26s - loss: 0.2782 - acc: 0.8840 - val_loss: 0.9385 - val_acc: 0.3498\n",
      "Epoch 13/50\n",
      "24s - loss: 0.2627 - acc: 0.8907 - val_loss: 0.9056 - val_acc: 0.3417\n",
      "Epoch 14/50\n",
      "25s - loss: 0.2522 - acc: 0.8897 - val_loss: 0.8417 - val_acc: 0.3575\n",
      "Epoch 15/50\n",
      "25s - loss: 0.2440 - acc: 0.8917 - val_loss: 0.7783 - val_acc: 0.4482\n",
      "Epoch 16/50\n",
      "26s - loss: 0.2435 - acc: 0.8955 - val_loss: 0.7909 - val_acc: 0.5010\n",
      "Epoch 17/50\n",
      "26s - loss: 0.2260 - acc: 0.8998 - val_loss: 0.8786 - val_acc: 0.3868\n",
      "Epoch 18/50\n",
      "28s - loss: 0.2180 - acc: 0.8984 - val_loss: 0.7627 - val_acc: 0.5240\n",
      "Epoch 19/50\n",
      "26s - loss: 0.2171 - acc: 0.9027 - val_loss: 0.8543 - val_acc: 0.4295\n",
      "Epoch 20/50\n",
      "25s - loss: 0.2086 - acc: 0.9108 - val_loss: 0.9933 - val_acc: 0.3695\n",
      "Epoch 21/50\n",
      "25s - loss: 0.1921 - acc: 0.9151 - val_loss: 0.9996 - val_acc: 0.3493\n",
      "Epoch 22/50\n",
      "24s - loss: 0.1850 - acc: 0.9166 - val_loss: 0.8861 - val_acc: 0.4381\n",
      "Epoch 23/50\n",
      "23s - loss: 0.1892 - acc: 0.9123 - val_loss: 1.0104 - val_acc: 0.3541\n",
      "Epoch 24/50\n",
      "27s - loss: 0.1840 - acc: 0.9199 - val_loss: 1.0455 - val_acc: 0.3599\n",
      "Epoch 25/50\n",
      "25s - loss: 0.1791 - acc: 0.9190 - val_loss: 0.9461 - val_acc: 0.3810\n",
      "Epoch 26/50\n",
      "26s - loss: 0.1706 - acc: 0.9214 - val_loss: 1.0128 - val_acc: 0.3642\n",
      "Epoch 27/50\n",
      "26s - loss: 0.1691 - acc: 0.9209 - val_loss: 1.1753 - val_acc: 0.3474\n",
      "Epoch 28/50\n",
      "25s - loss: 0.1646 - acc: 0.9204 - val_loss: 1.0593 - val_acc: 0.3719\n",
      "Epoch 29/50\n",
      "26s - loss: 0.1549 - acc: 0.9324 - val_loss: 1.2251 - val_acc: 0.3397\n",
      "Epoch 30/50\n",
      "26s - loss: 0.1564 - acc: 0.9329 - val_loss: 0.9145 - val_acc: 0.5038\n",
      "Epoch 31/50\n",
      "25s - loss: 0.1450 - acc: 0.9386 - val_loss: 0.9383 - val_acc: 0.4746\n",
      "Epoch 32/50\n",
      "25s - loss: 0.1372 - acc: 0.9391 - val_loss: 1.0077 - val_acc: 0.3709\n",
      "Epoch 33/50\n",
      "25s - loss: 0.1443 - acc: 0.9343 - val_loss: 0.9989 - val_acc: 0.4774\n",
      "Epoch 34/50\n",
      "24s - loss: 0.1314 - acc: 0.9463 - val_loss: 1.2700 - val_acc: 0.3388\n",
      "Epoch 35/50\n",
      "23s - loss: 0.1262 - acc: 0.9463 - val_loss: 1.2840 - val_acc: 0.3417\n",
      "Epoch 36/50\n",
      "19s - loss: 0.1181 - acc: 0.9497 - val_loss: 1.0387 - val_acc: 0.4443\n",
      "Epoch 37/50\n",
      "19s - loss: 0.1228 - acc: 0.9473 - val_loss: 1.3921 - val_acc: 0.3421\n",
      "Epoch 38/50\n",
      "19s - loss: 0.1176 - acc: 0.9439 - val_loss: 1.4617 - val_acc: 0.3373\n",
      "Epoch 39/50\n",
      "19s - loss: 0.1065 - acc: 0.9545 - val_loss: 1.3160 - val_acc: 0.3532\n",
      "Epoch 40/50\n",
      "19s - loss: 0.1052 - acc: 0.9559 - val_loss: 1.3638 - val_acc: 0.3570\n",
      "Epoch 41/50\n",
      "18s - loss: 0.0958 - acc: 0.9588 - val_loss: 1.8441 - val_acc: 0.3210\n",
      "Epoch 42/50\n",
      "19s - loss: 0.0959 - acc: 0.9602 - val_loss: 1.4744 - val_acc: 0.3580\n",
      "Epoch 43/50\n",
      "19s - loss: 0.0970 - acc: 0.9602 - val_loss: 1.6682 - val_acc: 0.3210\n",
      "Epoch 44/50\n",
      "21s - loss: 0.1032 - acc: 0.9559 - val_loss: 1.8278 - val_acc: 0.3215\n",
      "Epoch 45/50\n",
      "19s - loss: 0.0996 - acc: 0.9569 - val_loss: 1.7626 - val_acc: 0.3253\n",
      "Epoch 46/50\n",
      "19s - loss: 0.0876 - acc: 0.9684 - val_loss: 1.8422 - val_acc: 0.3369\n",
      "Epoch 47/50\n",
      "19s - loss: 0.0800 - acc: 0.9664 - val_loss: 1.9516 - val_acc: 0.3181\n",
      "Epoch 48/50\n",
      "19s - loss: 0.0682 - acc: 0.9760 - val_loss: 2.0345 - val_acc: 0.3325\n",
      "Epoch 49/50\n",
      "18s - loss: 0.0690 - acc: 0.9717 - val_loss: 2.0786 - val_acc: 0.3397\n",
      "Epoch 50/50\n",
      "18s - loss: 0.0738 - acc: 0.9688 - val_loss: 1.9888 - val_acc: 0.3234\n",
      "avg sec per epoch: 24.0741200018\n"
     ]
    }
   ],
   "source": [
    "# Forward pass LSTM network\n",
    "\n",
    "# try using different optimizers and different optimizer configs\n",
    "model_fdir_atom.compile('adam', 'binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "print('Train...')\n",
    "start_time = time.time()\n",
    "\n",
    "history_fdir_atom = model_fdir_atom.fit(X_train, y_train,\n",
    "                    batch_size=batch_size,\n",
    "                    nb_epoch=epochs,\n",
    "                    validation_data=[X_test, y_test], \n",
    "                    verbose=2)\n",
    "\n",
    "end_time = time.time()\n",
    "average_time_per_epoch = (end_time - start_time) / epochs\n",
    "print(\"avg sec per epoch:\", average_time_per_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "input_14 (InputLayer)            (None, 50)            0                                            \n",
      "____________________________________________________________________________________________________\n",
      "embedding_14 (Embedding)         (None, 50, 500)       5000000     input_14[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "batchnormalization_14 (BatchNorma(None, 50, 500)       1000        embedding_14[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "gru_7 (GRU)                      (None, 100)           180300      batchnormalization_14[0][0]      \n",
      "____________________________________________________________________________________________________\n",
      "gru_8 (GRU)                      (None, 100)           180300      batchnormalization_14[0][0]      \n",
      "____________________________________________________________________________________________________\n",
      "merge_4 (Merge)                  (None, 200)           0           gru_7[0][0]                      \n",
      "                                                                   gru_8[0][0]                      \n",
      "____________________________________________________________________________________________________\n",
      "dropout_14 (Dropout)             (None, 200)           0           merge_4[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "dense_14 (Dense)                 (None, 1)             201         dropout_14[0][0]                 \n",
      "====================================================================================================\n",
      "Total params: 5361801\n",
      "____________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Bi-directional Atom\n",
    "\n",
    "# based on keras tutorial: https://github.com/fchollet/keras/blob/master/examples/imdb_bidirectional_lstm.py\n",
    "\n",
    "# this is the placeholder tensor for the input sequences\n",
    "sequence = Input(shape=(max_len,), dtype='int32')\n",
    "# this embedding layer will transform the sequences of integers\n",
    "# into vectors of size embedding\n",
    "# embedding layer converts dense int input to one-hot in real time to save memory\n",
    "embedded = Embedding(max_features, embedding_neurons, input_length=max_len)(sequence)\n",
    "# normalize embeddings by input/word in sentence\n",
    "bnorm = BatchNormalization()(embedded)\n",
    "\n",
    "# apply forwards LSTM layer size lstm_neurons\n",
    "forwards = GRU(lstm_neurons, dropout_W=0.4, dropout_U=0.4)(bnorm)\n",
    "# apply backwards LSTM\n",
    "backwards = GRU(lstm_neurons, dropout_W=0.4, dropout_U=0.4, go_backwards=True)(bnorm)\n",
    "\n",
    "# concatenate the outputs of the 2 LSTMs\n",
    "merged = merge([forwards, backwards], mode='concat', concat_axis=-1)\n",
    "after_dp = Dropout(0.5)(merged)\n",
    "output = Dense(1, activation='sigmoid')(after_dp)\n",
    "\n",
    "model_bidir_atom = Model(input=sequence, output=output)\n",
    "# review model structure\n",
    "print(model_bidir_atom.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train...\n",
      "Train on 2086 samples, validate on 2084 samples\n",
      "Epoch 1/50\n",
      "24s - loss: 0.7475 - acc: 0.6496 - val_loss: 0.6160 - val_acc: 0.7179\n",
      "Epoch 2/50\n",
      "24s - loss: 0.6852 - acc: 0.6716 - val_loss: 0.6185 - val_acc: 0.7116\n",
      "Epoch 3/50\n",
      "24s - loss: 0.6529 - acc: 0.6779 - val_loss: 0.6116 - val_acc: 0.7073\n",
      "Epoch 4/50\n",
      "24s - loss: 0.5700 - acc: 0.7258 - val_loss: 0.7040 - val_acc: 0.7164\n",
      "Epoch 5/50\n",
      "25s - loss: 0.4870 - acc: 0.7723 - val_loss: 0.6169 - val_acc: 0.7039\n",
      "Epoch 6/50\n",
      "24s - loss: 0.4164 - acc: 0.8154 - val_loss: 0.6212 - val_acc: 0.7102\n",
      "Epoch 7/50\n",
      "25s - loss: 0.3553 - acc: 0.8452 - val_loss: 0.6796 - val_acc: 0.7150\n",
      "Epoch 8/50\n",
      "25s - loss: 0.3265 - acc: 0.8615 - val_loss: 0.7790 - val_acc: 0.7169\n",
      "Epoch 9/50\n",
      "25s - loss: 0.3144 - acc: 0.8663 - val_loss: 0.7623 - val_acc: 0.7155\n",
      "Epoch 10/50\n",
      "25s - loss: 0.2865 - acc: 0.8754 - val_loss: 0.7952 - val_acc: 0.7145\n",
      "Epoch 11/50\n",
      "24s - loss: 0.2938 - acc: 0.8682 - val_loss: 0.7378 - val_acc: 0.7121\n",
      "Epoch 12/50\n",
      "24s - loss: 0.2746 - acc: 0.8763 - val_loss: 0.7574 - val_acc: 0.7135\n",
      "Epoch 13/50\n",
      "24s - loss: 0.2813 - acc: 0.8778 - val_loss: 0.6405 - val_acc: 0.6987\n",
      "Epoch 14/50\n",
      "24s - loss: 0.2826 - acc: 0.8782 - val_loss: 0.8708 - val_acc: 0.7155\n",
      "Epoch 15/50\n",
      "25s - loss: 0.2680 - acc: 0.8782 - val_loss: 0.9037 - val_acc: 0.7179\n",
      "Epoch 16/50\n",
      "25s - loss: 0.2718 - acc: 0.8806 - val_loss: 0.9277 - val_acc: 0.7179\n",
      "Epoch 17/50\n",
      "25s - loss: 0.2530 - acc: 0.8821 - val_loss: 0.6842 - val_acc: 0.7107\n",
      "Epoch 18/50\n",
      "25s - loss: 0.2643 - acc: 0.8778 - val_loss: 0.6703 - val_acc: 0.7131\n",
      "Epoch 19/50\n",
      "25s - loss: 0.2411 - acc: 0.8912 - val_loss: 0.6787 - val_acc: 0.7131\n",
      "Epoch 20/50\n",
      "25s - loss: 0.2588 - acc: 0.8830 - val_loss: 0.7064 - val_acc: 0.7140\n",
      "Epoch 21/50\n",
      "24s - loss: 0.2468 - acc: 0.8816 - val_loss: 0.6986 - val_acc: 0.7131\n",
      "Epoch 22/50\n",
      "24s - loss: 0.2340 - acc: 0.8854 - val_loss: 0.7202 - val_acc: 0.7145\n",
      "Epoch 23/50\n",
      "24s - loss: 0.2438 - acc: 0.8849 - val_loss: 0.7398 - val_acc: 0.7140\n",
      "Epoch 24/50\n",
      "24s - loss: 0.2418 - acc: 0.8835 - val_loss: 0.7263 - val_acc: 0.7111\n",
      "Epoch 25/50\n",
      "25s - loss: 0.2249 - acc: 0.8969 - val_loss: 0.7284 - val_acc: 0.7102\n",
      "Epoch 26/50\n",
      "24s - loss: 0.2308 - acc: 0.8936 - val_loss: 0.7280 - val_acc: 0.7102\n",
      "Epoch 27/50\n",
      "24s - loss: 0.2241 - acc: 0.8921 - val_loss: 0.7082 - val_acc: 0.7087\n",
      "Epoch 28/50\n",
      "24s - loss: 0.2239 - acc: 0.8960 - val_loss: 0.7197 - val_acc: 0.7087\n",
      "Epoch 29/50\n",
      "24s - loss: 0.2105 - acc: 0.9041 - val_loss: 0.7204 - val_acc: 0.6991\n",
      "Epoch 30/50\n",
      "25s - loss: 0.2249 - acc: 0.8974 - val_loss: 0.7136 - val_acc: 0.7015\n",
      "Epoch 31/50\n",
      "26s - loss: 0.2240 - acc: 0.8945 - val_loss: 0.6728 - val_acc: 0.6809\n",
      "Epoch 32/50\n",
      "25s - loss: 0.2259 - acc: 0.8888 - val_loss: 0.6824 - val_acc: 0.6646\n",
      "Epoch 33/50\n",
      "26s - loss: 0.2170 - acc: 0.8969 - val_loss: 0.7095 - val_acc: 0.6972\n",
      "Epoch 34/50\n",
      "26s - loss: 0.2123 - acc: 0.8902 - val_loss: 0.6955 - val_acc: 0.6910\n",
      "Epoch 35/50\n",
      "26s - loss: 0.2042 - acc: 0.9070 - val_loss: 0.7054 - val_acc: 0.6852\n",
      "Epoch 36/50\n",
      "25s - loss: 0.2087 - acc: 0.9032 - val_loss: 0.6934 - val_acc: 0.6795\n",
      "Epoch 37/50\n",
      "25s - loss: 0.2019 - acc: 0.9012 - val_loss: 0.7173 - val_acc: 0.6905\n",
      "Epoch 38/50\n",
      "25s - loss: 0.1960 - acc: 0.9080 - val_loss: 0.6818 - val_acc: 0.6833\n",
      "Epoch 39/50\n",
      "26s - loss: 0.2047 - acc: 0.9046 - val_loss: 0.7211 - val_acc: 0.7020\n",
      "Epoch 40/50\n",
      "25s - loss: 0.2079 - acc: 0.8960 - val_loss: 0.6879 - val_acc: 0.6819\n",
      "Epoch 41/50\n",
      "25s - loss: 0.1976 - acc: 0.9099 - val_loss: 0.7222 - val_acc: 0.6982\n",
      "Epoch 42/50\n",
      "26s - loss: 0.1916 - acc: 0.9070 - val_loss: 0.6877 - val_acc: 0.6713\n",
      "Epoch 43/50\n",
      "26s - loss: 0.2007 - acc: 0.9056 - val_loss: 0.6944 - val_acc: 0.6814\n",
      "Epoch 44/50\n",
      "25s - loss: 0.1865 - acc: 0.9108 - val_loss: 0.6997 - val_acc: 0.6780\n",
      "Epoch 45/50\n",
      "26s - loss: 0.1847 - acc: 0.9113 - val_loss: 0.7120 - val_acc: 0.6612\n",
      "Epoch 46/50\n",
      "25s - loss: 0.1859 - acc: 0.9118 - val_loss: 0.7165 - val_acc: 0.6598\n",
      "Epoch 47/50\n",
      "27s - loss: 0.1806 - acc: 0.9151 - val_loss: 0.7186 - val_acc: 0.6819\n",
      "Epoch 48/50\n",
      "25s - loss: 0.1784 - acc: 0.9108 - val_loss: 0.7090 - val_acc: 0.6545\n",
      "Epoch 49/50\n",
      "25s - loss: 0.1900 - acc: 0.9084 - val_loss: 0.7380 - val_acc: 0.6857\n",
      "Epoch 50/50\n",
      "25s - loss: 0.1841 - acc: 0.9132 - val_loss: 0.7148 - val_acc: 0.6790\n",
      "avg sec per epoch: 25.9025\n"
     ]
    }
   ],
   "source": [
    "# Bi-directional Atom\n",
    "\n",
    "# try using different optimizers and different optimizer configs\n",
    "model_bidir_atom.compile('adam', 'binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "print('Train...')\n",
    "start_time = time.time()\n",
    "\n",
    "history_bidir_atom = model_bidir_atom.fit(X_train, y_train,\n",
    "                    batch_size=batch_size,\n",
    "                    nb_epoch=epochs,\n",
    "                    validation_data=[X_test, y_test], \n",
    "                    verbose=2)\n",
    "\n",
    "end_time = time.time()\n",
    "average_time_per_epoch = (end_time - start_time) / epochs\n",
    "print(\"avg sec per epoch:\", average_time_per_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
