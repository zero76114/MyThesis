{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Testing mode: consume_less=\"cpu\"\n",
      "Train on 35856 samples, validate on 8964 samples\n",
      "Epoch 1/10\n",
      "35856/35856 [==============================] - 1808s - loss: 0.6804 - acc: 0.5739 - val_loss: 0.6769 - val_acc: 0.5747\n",
      "Epoch 2/10\n",
      "35856/35856 [==============================] - 745s - loss: 0.6571 - acc: 0.6040 - val_loss: 0.6796 - val_acc: 0.5701\n",
      "Epoch 3/10\n",
      "35856/35856 [==============================] - 403s - loss: 0.5989 - acc: 0.6745 - val_loss: 0.7041 - val_acc: 0.5591\n",
      "Epoch 4/10\n",
      "35856/35856 [==============================] - 410s - loss: 0.5232 - acc: 0.7376 - val_loss: 0.7612 - val_acc: 0.5370\n",
      "Epoch 5/10\n",
      "35856/35856 [==============================] - 425s - loss: 0.4509 - acc: 0.7868 - val_loss: 0.8414 - val_acc: 0.5486\n",
      "Epoch 6/10\n",
      "35856/35856 [==============================] - 409s - loss: 0.3919 - acc: 0.8197 - val_loss: 0.9336 - val_acc: 0.5419\n",
      "Epoch 7/10\n",
      "35856/35856 [==============================] - 420s - loss: 0.3390 - acc: 0.8490 - val_loss: 1.0188 - val_acc: 0.5488\n",
      "Epoch 8/10\n",
      "35856/35856 [==============================] - 403s - loss: 0.2972 - acc: 0.8718 - val_loss: 1.1318 - val_acc: 0.5473\n",
      "Epoch 9/10\n",
      "35856/35856 [==============================] - 403s - loss: 0.2647 - acc: 0.8864 - val_loss: 1.2092 - val_acc: 0.5395\n",
      "Epoch 10/10\n",
      "35856/35856 [==============================] - 409s - loss: 0.2378 - acc: 0.9003 - val_loss: 1.2202 - val_acc: 0.5376\n",
      "Testing mode: consume_less=\"mem\"\n",
      "Train on 35856 samples, validate on 8964 samples\n",
      "Epoch 1/10\n",
      "35856/35856 [==============================] - 357s - loss: 0.6808 - acc: 0.5729 - val_loss: 0.6820 - val_acc: 0.5727\n",
      "Epoch 2/10\n",
      "35856/35856 [==============================] - 351s - loss: 0.6614 - acc: 0.5971 - val_loss: 0.6780 - val_acc: 0.5728\n",
      "Epoch 3/10\n",
      "35856/35856 [==============================] - 355s - loss: 0.6150 - acc: 0.6561 - val_loss: 0.7062 - val_acc: 0.5653\n",
      "Epoch 4/10\n",
      "35856/35856 [==============================] - 351s - loss: 0.5551 - acc: 0.7128 - val_loss: 0.7260 - val_acc: 0.5604\n",
      "Epoch 5/10\n",
      "35856/35856 [==============================] - 361s - loss: 0.4967 - acc: 0.7530 - val_loss: 0.8143 - val_acc: 0.5506\n",
      "Epoch 6/10\n",
      "35856/35856 [==============================] - 351s - loss: 0.4340 - acc: 0.7959 - val_loss: 0.8835 - val_acc: 0.5513\n",
      "Epoch 7/10\n",
      "35856/35856 [==============================] - 367s - loss: 0.3772 - acc: 0.8275 - val_loss: 0.9223 - val_acc: 0.5443\n",
      "Epoch 8/10\n",
      "35856/35856 [==============================] - 351s - loss: 0.3331 - acc: 0.8509 - val_loss: 1.0086 - val_acc: 0.5531\n",
      "Epoch 9/10\n",
      "35856/35856 [==============================] - 370s - loss: 0.3016 - acc: 0.8666 - val_loss: 1.0662 - val_acc: 0.5406\n",
      "Epoch 10/10\n",
      "35856/35856 [==============================] - 356s - loss: 0.2764 - acc: 0.8807 - val_loss: 1.1300 - val_acc: 0.5483\n",
      "Testing mode: consume_less=\"gpu\"\n",
      "Train on 35856 samples, validate on 8964 samples\n",
      "Epoch 1/10\n",
      "35856/35856 [==============================] - 293s - loss: 0.6793 - acc: 0.5756 - val_loss: 0.6766 - val_acc: 0.5704\n",
      "Epoch 2/10\n",
      "35856/35856 [==============================] - 298s - loss: 0.6546 - acc: 0.6087 - val_loss: 0.6877 - val_acc: 0.5679\n",
      "Epoch 3/10\n",
      "35856/35856 [==============================] - 298s - loss: 0.5967 - acc: 0.6718 - val_loss: 0.7048 - val_acc: 0.5601\n",
      "Epoch 4/10\n",
      "35856/35856 [==============================] - 295s - loss: 0.5360 - acc: 0.7254 - val_loss: 0.7723 - val_acc: 0.5477\n",
      "Epoch 5/10\n",
      "35856/35856 [==============================] - 306s - loss: 0.4755 - acc: 0.7682 - val_loss: 0.8122 - val_acc: 0.5409\n",
      "Epoch 6/10\n",
      "35856/35856 [==============================] - 301s - loss: 0.4185 - acc: 0.8025 - val_loss: 0.8797 - val_acc: 0.5492\n",
      "Epoch 7/10\n",
      "35856/35856 [==============================] - 304s - loss: 0.3713 - acc: 0.8302 - val_loss: 0.9549 - val_acc: 0.5523\n",
      "Epoch 8/10\n",
      "35856/35856 [==============================] - 310s - loss: 0.3334 - acc: 0.8501 - val_loss: 0.9914 - val_acc: 0.5567\n",
      "Epoch 9/10\n",
      "35856/35856 [==============================] - 298s - loss: 0.2983 - acc: 0.8665 - val_loss: 1.0928 - val_acc: 0.5432\n",
      "Epoch 10/10\n",
      "35856/35856 [==============================] - 306s - loss: 0.2728 - acc: 0.8805 - val_loss: 1.2163 - val_acc: 0.5427\n"
     ]
    }
   ],
   "source": [
    "'''Compare LSTM implementations on the IMDB sentiment classification task.\n",
    "consume_less='cpu' preprocesses input to the LSTM which typically results in\n",
    "faster computations at the expense of increased peak memory usage as the\n",
    "preprocessed input must be kept in memory.\n",
    "consume_less='mem' does away with the preprocessing, meaning that it might take\n",
    "a little longer, but should require less peak memory.\n",
    "consume_less='gpu' concatenates the input, output and forget gate's weights\n",
    "into one, large matrix, resulting in faster computation time as the GPU can\n",
    "utilize more cores, at the expense of reduced regularization because the same\n",
    "dropout is shared across the gates.\n",
    "Note that the relative performance of the different `consume_less` modes\n",
    "can vary depending on your device, your model and the size of your data.\n",
    "'''\n",
    "\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, Dense, LSTM\n",
    "from keras.datasets import imdb\n",
    "\n",
    "max_features = 20000\n",
    "max_length = 80\n",
    "embedding_dim = 256\n",
    "batch_size = 128\n",
    "epochs = 10\n",
    "modes = ['cpu', 'mem', 'gpu']\n",
    "\n",
    "print('Loading data...')\n",
    "(X_train, y_train), (X_test, y_test) = imdb.load_data(path='C:/1_Research/Create_data/aclImdb/reuter.pkl',nb_words=max_features,\n",
    "                                                      test_split=0.2)\n",
    "X_train = sequence.pad_sequences(X_train, max_length)\n",
    "X_test = sequence.pad_sequences(X_test, max_length)\n",
    "\n",
    "# Compile and train different models while meauring performance.\n",
    "results = []\n",
    "for mode in modes:\n",
    "    print('Testing mode: consume_less=\"{}\"'.format(mode))\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(max_features, embedding_dim, input_length=max_length, dropout=0.2))\n",
    "    model.add(LSTM(embedding_dim, dropout_W=0.2, dropout_U=0.2, consume_less=mode))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    start_time = time.time()\n",
    "    history = model.fit(X_train, y_train,\n",
    "                        batch_size=batch_size,\n",
    "                        nb_epoch=epochs,\n",
    "                        validation_data=(X_test, y_test))\n",
    "    average_time_per_epoch = (time.time() - start_time) / epochs\n",
    "\n",
    "    results.append((history, average_time_per_epoch))\n",
    "\n",
    "# Compare models' accuracy, loss and elapsed time per epoch.\n",
    "plt.style.use('ggplot')\n",
    "ax1 = plt.subplot2grid((2, 2), (0, 0))\n",
    "ax1.set_title('Accuracy')\n",
    "ax1.set_ylabel('Validation Accuracy')\n",
    "ax1.set_xlabel('Epochs')\n",
    "ax2 = plt.subplot2grid((2, 2), (1, 0))\n",
    "ax2.set_title('Loss')\n",
    "ax2.set_ylabel('Validation Loss')\n",
    "ax2.set_xlabel('Epochs')\n",
    "ax3 = plt.subplot2grid((2, 2), (0, 1), rowspan=2)\n",
    "ax3.set_title('Time')\n",
    "ax3.set_ylabel('Seconds')\n",
    "for mode, result in zip(modes, results):\n",
    "    ax1.plot(result[0].epoch, result[0].history['val_acc'], label=mode)\n",
    "    ax2.plot(result[0].epoch, result[0].history['val_loss'], label=mode)\n",
    "ax1.legend()\n",
    "ax2.legend()\n",
    "ax3.bar(np.arange(len(results)), [x[1] for x in results],\n",
    "        tick_label=modes, align='center')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
